# CIP-XXXX: Hybrid State Synchronization Protocol

| Field | Value |
|-------|-------|
| CIP | XXXX (To be assigned) |
| Title | Hybrid State Synchronization Protocol |
| Author | Calimero Team |
| Status | Draft |
| Type | Standards Track |
| Category | Core |
| Created | 2026-01-30 |

## Abstract

This CIP proposes a hybrid synchronization protocol that combines delta-based (CmRDT) and state-based (CvRDT) approaches to efficiently synchronize Merkle tree state between nodes. The protocol:

1. **Automatically selects** the optimal sync strategy based on divergence characteristics
2. **Maintains node liveness** during sync operations via delta buffering
3. **Ensures cryptographic verification** of synchronized state
4. **Implements hybrid merge dispatch** where built-in CRDTs merge in storage layer (fast, ~100ns) and custom Mergeable types dispatch to WASM (flexible, ~10Î¼s)

## Motivation

The current synchronization implementation has several limitations:

1. **Fresh Node Bootstrap**: New nodes must fetch ALL deltas from genesis, which is inefficient for contexts with long history (thousands of deltas).

2. **Missing Delta Recovery**: When too many deltas are missing (network partition, offline period), delta-based sync becomes impractical.

3. **No Protocol Selection**: There's no mechanism to choose between different sync strategies based on the situation.

4. **Sync Blocking**: The relationship between ongoing sync and incoming deltas is not well-defined, risking state inconsistency.

5. **No State Verification**: Snapshot transfers don't have cryptographic verification against Merkle root hashes.

6. **CRDT Merge Not Used in State Sync**: State-based sync uses Last-Write-Wins (LWW) instead of proper CRDT merge semantics, causing data loss when concurrent updates occur on built-in CRDTs (Counter, Map, etc.).

7. **Custom Merge Logic Inaccessible**: Apps can define custom `Mergeable` implementations in WASM, but state sync cannot invoke them - it always falls back to LWW.

### Use Cases

| Scenario | Current Behavior | Proposed Behavior |
|----------|------------------|-------------------|
| Fresh node joins | Fetch ALL deltas recursively | Snapshot sync with verification |
| 1% divergence | Fetch missing deltas | Hash-based incremental sync |
| 50% divergence | Fetch ALL missing deltas | Snapshot sync (more efficient) |
| Network partition recovery | May timeout/fail | Adaptive protocol selection |
| Malicious snapshot | Blindly accepted | Cryptographic verification |
| Counter conflict (state sync) | LWW - **data loss!** | Sum per-node counts (CRDT merge) |
| Map conflict (state sync) | LWW - **data loss!** | Per-key merge (preserves all keys) |
| Custom type conflict | LWW only | WASM callback for app-defined merge |
| Root state conflict | LWW | WASM merge_root_state callback |

## Specification

### 1. Sync Protocol Types

```rust
pub enum SyncProtocol {
    /// No sync needed - already in sync
    None,
    
    /// Delta-based sync via DAG (existing)
    DeltaSync {
        missing_delta_ids: Vec<[u8; 32]>,
    },
    
    /// Hash-based Merkle tree comparison
    HashComparison {
        root_hash: [u8; 32],
        divergent_subtrees: Vec<Id>,
    },
    
    /// Full state snapshot transfer
    Snapshot {
        compressed: bool,
        verified: bool,
    },
    
    /// Bloom filter quick diff
    BloomFilter {
        filter_size: usize,
        false_positive_rate: f64,
    },
    
    /// Subtree prefetch for deep localized changes
    SubtreePrefetch {
        subtree_roots: Vec<Id>,
    },
    
    /// Level-wise sync for wide shallow trees
    LevelWise {
        max_depth: usize,
    },
}
```

### 2. Protocol Negotiation

#### 2.1 Handshake Message

```rust
pub struct SyncHandshake {
    /// Our current root hash
    pub root_hash: [u8; 32],
    
    /// Whether we have any state
    pub has_state: bool,
    
    /// Number of entities in our tree
    pub entity_count: usize,
    
    /// Maximum tree depth
    pub max_depth: usize,
    
    /// Our DAG heads (for delta sync compatibility)
    pub dag_heads: Vec<[u8; 32]>,
    
    /// Supported protocols (ordered by preference)
    pub supported_protocols: Vec<SyncProtocol>,
}
```

#### 2.2 Negotiation Flow

```
Requester                              Responder
    â”‚                                      â”‚
    â”‚â”€â”€â”€â”€ SyncHandshake â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚                                      â”‚
    â”‚<â”€â”€â”€ SyncHandshake â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                                      â”‚
    â”‚     (Both compute optimal protocol)  â”‚
    â”‚                                      â”‚
    â”‚â”€â”€â”€â”€ ProtocolSelected { protocol } â”€â”€>â”‚
    â”‚                                      â”‚
    â”‚<â”€â”€â”€ ProtocolAck / ProtocolNack â”€â”€â”€â”€â”€â”€â”‚
    â”‚                                      â”‚
    â”‚     (Begin selected protocol)        â”‚
```

#### 2.3 Protocol Selection Algorithm

```rust
fn select_protocol(local: &SyncHandshake, remote: &SyncHandshake) -> SyncProtocol {
    // Already in sync
    if local.root_hash == remote.root_hash {
        return SyncProtocol::None;
    }
    
    // Helper to check if remote supports a protocol
    let remote_supports = |p: &SyncProtocol| -> bool {
        remote.supported_protocols.iter().any(|sp| 
            std::mem::discriminant(sp) == std::mem::discriminant(p)
        )
    };
    
    // Fresh node (no state) - use snapshot
    if !local.has_state {
        let preferred = if remote.entity_count > 100 {
            SyncProtocol::Snapshot { compressed: true, verified: true }
        } else {
            SyncProtocol::Snapshot { compressed: false, verified: true }
        };
        
        // Check if remote supports snapshot, fallback to delta if not
        if remote_supports(&preferred) {
            return preferred;
        } else {
            // Fallback: delta sync (always supported)
            return SyncProtocol::DeltaSync { missing_delta_ids: vec![] };
        }
    }
    
    // Calculate divergence estimate
    let count_diff = (local.entity_count as i64 - remote.entity_count as i64).abs();
    let divergence_ratio = count_diff as f32 / remote.entity_count.max(1) as f32;
    
    // Large divergence (>50%) - snapshot is more efficient
    if divergence_ratio > 0.5 && remote.entity_count > 20 {
        let preferred = SyncProtocol::Snapshot { 
            compressed: remote.entity_count > 100,
            verified: true,
        };
        if remote_supports(&preferred) {
            return preferred;
        }
        // Fallback to hash comparison
    }
    
    // Deep tree with localized changes - subtree prefetch
    if remote.max_depth > 3 && divergence_ratio < 0.2 {
        let preferred = SyncProtocol::SubtreePrefetch { subtree_roots: vec![] };
        if remote_supports(&preferred) {
            return preferred;
        }
    }
    
    // Large tree with small diff - Bloom filter
    if remote.entity_count > 50 && divergence_ratio < 0.1 {
        let preferred = SyncProtocol::BloomFilter {
            filter_size: local.entity_count * 10,
            false_positive_rate: 0.01,
        };
        if remote_supports(&preferred) {
            return preferred;
        }
    }
    
    // Wide shallow tree - level-wise
    if remote.max_depth <= 2 {
        let preferred = SyncProtocol::LevelWise { max_depth: remote.max_depth };
        if remote_supports(&preferred) {
            return preferred;
        }
    }
    
    // Default: hash-based comparison (always supported as baseline)
    let hash_sync = SyncProtocol::HashComparison {
        root_hash: local.root_hash,
        divergent_subtrees: vec![],
    };
    
    if remote_supports(&hash_sync) {
        return hash_sync;
    }
    
    // Final fallback: delta sync (guaranteed supported by all nodes)
    SyncProtocol::DeltaSync { missing_delta_ids: vec![] }
}
```

### 3. Sync Hints in Delta Propagation

When a node applies a local delta and propagates it, include **sync hints** to help receivers decide proactively if they need a full sync instead of waiting to discover divergence.

#### 3.1 Enhanced Delta Message

```rust
pub struct DeltaWithHints {
    /// The actual delta
    pub delta: CausalDelta,
    
    /// Sync hints for receivers
    pub hints: SyncHints,
}

pub struct SyncHints {
    /// Current root hash after applying this delta
    pub root_hash: [u8; 32],
    
    /// Total entity count in tree
    pub entity_count: usize,
    
    /// How many deltas since genesis (chain height)
    pub delta_height: u64,
    
    /// Number of deltas in last N minutes (activity indicator)
    pub recent_delta_count: u32,
    
    /// Bloom filter of all delta IDs we have
    /// (compact way to detect missing deltas)
    pub delta_bloom_filter: Option<Vec<u8>>,
    
    /// Estimated "age" - oldest missing ancestor we know about
    pub oldest_pending_parent: Option<[u8; 32]>,
}
```

#### 3.2 Receiver Decision Logic

When a node receives a delta with hints, it can immediately determine its sync strategy:

```rust
impl SyncManager {
    fn on_delta_received(&mut self, msg: DeltaWithHints) -> SyncDecision {
        let hints = &msg.hints;
        
        // 1. Check if we're already in sync
        if self.root_hash == hints.root_hash {
            return SyncDecision::AlreadySynced;
        }
        
        // 2. Check if we have the parent deltas
        let missing_parents: Vec<[u8; 32]> = msg.delta.parents
            .iter()
            .filter(|p| !self.dag_store.has_delta(p))
            .copied()
            .collect();
        
        if !missing_parents.is_empty() {
            // Missing parents - how many?
            let our_height = self.dag_store.height();
            let gap = hints.delta_height.saturating_sub(our_height);
            
            if gap > DELTA_SYNC_THRESHOLD {
                // Too far behind - request snapshot instead of chasing deltas
                return SyncDecision::RequestSnapshot {
                    peer: msg.sender,
                    reason: SyncReason::TooFarBehind { gap },
                };
            }
            
            // Small gap - request missing parent deltas first
            return SyncDecision::RequestMissingDeltas {
                delta_ids: missing_parents,
            };
        }
        
        // 3. Use bloom filter to estimate missing deltas
        if let Some(ref bloom) = hints.delta_bloom_filter {
            let missing_estimate = self.estimate_missing_from_bloom(bloom);
            
            if missing_estimate > DELTA_SYNC_THRESHOLD {
                return SyncDecision::RequestSnapshot {
                    peer: msg.sender,
                    reason: SyncReason::TooManyMissing { estimate: missing_estimate },
                };
            }
        }
        
        // 4. Entity count divergence check
        let our_count = self.entity_count();
        let count_diff = (our_count as i64 - hints.entity_count as i64).abs();
        let divergence = count_diff as f32 / hints.entity_count.max(1) as f32;
        
        if divergence > 0.5 {
            return SyncDecision::RequestHashSync {
                peer: msg.sender,
                reason: SyncReason::SignificantDivergence { ratio: divergence },
            };
        }
        
        // 5. All parents present - safe to apply
        SyncDecision::ApplyDelta(msg.delta)
    }
}

pub enum SyncDecision {
    AlreadySynced,
    ApplyDelta(CausalDelta),
    RequestMissingDeltas { delta_ids: Vec<[u8; 32]> },
    RequestHashSync { peer: PeerId, reason: SyncReason },
    RequestSnapshot { peer: PeerId, reason: SyncReason },
}

pub enum SyncReason {
    TooFarBehind { gap: u64 },
    TooManyMissing { estimate: usize },
    SignificantDivergence { ratio: f32 },
    FreshNode,
}
```

#### 3.3 Lightweight Hints (Minimal Overhead)

For nodes concerned about bandwidth, a minimal hint set:

```rust
pub struct LightweightHints {
    /// Just the root hash - receivers can compare
    pub root_hash: [u8; 32],
    
    /// Delta height - single number to detect gaps
    pub delta_height: u64,
}
```

**Overhead:** Only 40 bytes per delta propagation.

#### 3.4 Proactive Sync Triggers

With hints, sync can be triggered **proactively** instead of reactively:

| Trigger | Without Hints | With Hints |
|---------|---------------|------------|
| Fresh node joins | Waits for first delta, then discovers gap | Immediately sees `delta_height` gap |
| Network partition heals | Tries delta sync, times out, then retries | Sees `root_hash` mismatch + `delta_height` gap |
| Slow node catches up | Recursively fetches deltas one by one | Sees gap > threshold, requests snapshot |
| Malicious delta | Applies, then discovers state mismatch | Verifies `root_hash` matches expected |

#### 3.5 Gossip Protocol Enhancement

Delta gossip can include hints at different verbosity levels:

```rust
pub enum GossipMode {
    /// Just the delta (current behavior)
    DeltaOnly,
    
    /// Delta + lightweight hints (40 bytes extra)
    WithLightHints,
    
    /// Delta + full hints (for nodes returning from offline)
    WithFullHints,
    
    /// Periodic announcement of state (no delta, just hints)
    StateAnnouncement,
}
```

**State Announcements** allow nodes to periodically broadcast their state summary, enabling peers to detect divergence even without active delta propagation:

```rust
impl SyncManager {
    /// Periodic state announcement (e.g., every 30 seconds)
    fn announce_state(&self) {
        let announcement = SyncHints {
            root_hash: self.root_hash,
            entity_count: self.entity_count(),
            delta_height: self.dag_store.height(),
            recent_delta_count: self.recent_delta_count(),
            delta_bloom_filter: Some(self.dag_store.bloom_filter()),
            oldest_pending_parent: None,
        };
        
        self.network.gossip(GossipMessage::StateAnnouncement(announcement));
    }
}
```

### 4. Sync State Machine

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     IDLE        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ sync_trigger()
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   NEGOTIATING   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ protocol_selected()
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DELTA_SYNCING â”‚   â”‚ STATE_SYNCING â”‚   â”‚ HASH_SYNCING  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â”‚   sync_complete() â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   VERIFYING     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ verification_passed()
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   APPLYING      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ apply_complete()
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     IDLE        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Delta Handling During Sync

#### 4.1 Delta Buffer

During state-based sync (snapshot, hash comparison), incoming deltas MUST be buffered:

```rust
pub struct SyncContext {
    /// Current sync state
    state: SyncState,
    
    /// Deltas received during sync (buffered)
    buffered_deltas: Vec<CausalDelta>,
    
    /// Snapshot of root hash when sync started
    sync_start_root_hash: [u8; 32],
    
    /// HLC timestamp when sync started (for filtering buffered deltas)
    sync_start_hlc: HybridTimestamp,
    
    /// Root hash received from peer
    peer_root_hash: [u8; 32],
    
    /// DAG store reference
    dag_store: DagStore,
}

impl SyncContext {
    /// Handle incoming delta during sync
    pub fn on_delta_received(&mut self, delta: CausalDelta) {
        match self.state {
            SyncState::Idle => {
                // Normal operation - apply immediately
                self.dag_store.add_delta(delta);
            }
            SyncState::DeltaSyncing => {
                // Delta sync in progress - add to DAG (may go pending)
                self.dag_store.add_delta(delta);
            }
            SyncState::StateSyncing | SyncState::HashSyncing => {
                // State-based sync - BUFFER for later
                self.buffered_deltas.push(delta);
            }
            SyncState::Verifying | SyncState::Applying => {
                // Buffer until sync completes
                self.buffered_deltas.push(delta);
            }
        }
    }
}
```

#### 4.2 Post-Sync Delta Replay

After state-based sync completes:

```rust
impl SyncContext {
    pub async fn finalize_sync(&mut self) -> Result<()> {
        // 1. Verify received state
        self.verify_snapshot()?;
        
        // 2. Apply received state
        self.apply_snapshot()?;
        
        // 3. Replay buffered deltas in order
        self.buffered_deltas.sort_by(|a, b| a.hlc.cmp(&b.hlc));
        
        for delta in self.buffered_deltas.drain(..) {
            // Deltas authored AFTER sync started should be applied
            // Deltas authored BEFORE are already in snapshot
            if delta.hlc > self.sync_start_hlc {
                self.dag_store.add_delta(delta).await?;
            }
        }
        
        // 4. Transition to idle
        self.state = SyncState::Idle;
        
        Ok(())
    }
}
```

### 6. Cryptographic Verification

#### 5.1 Snapshot Verification

```rust
impl Snapshot {
    /// Verify all entity hashes match their index entries
    pub fn verify(&self) -> Result<(), VerificationError> {
        for (id, data) in &self.entries {
            // Compute hash of entity data
            let computed_hash = sha256(data);
            
            // Find corresponding index entry
            let index_entry = self.indexes.iter()
                .find(|idx| idx.id() == *id)
                .ok_or(VerificationError::MissingIndex(*id))?;
            
            // Verify hash matches
            if computed_hash != index_entry.own_hash() {
                return Err(VerificationError::HashMismatch {
                    id: *id,
                    expected: index_entry.own_hash(),
                    computed: computed_hash,
                });
            }
        }
        
        // Verify root hash
        let computed_root = self.compute_root_hash();
        if computed_root != self.root_hash {
            return Err(VerificationError::RootHashMismatch {
                expected: self.root_hash,
                computed: computed_root,
            });
        }
        
        Ok(())
    }
}
```

#### 5.2 Incremental Verification

For hash-based sync, verify each entity as received:

```rust
fn verify_entity(
    id: Id,
    data: &[u8],
    comparison: &ComparisonData,
) -> Result<(), VerificationError> {
    let computed_own_hash = sha256(data);
    
    if computed_own_hash != comparison.own_hash {
        return Err(VerificationError::HashMismatch {
            id,
            expected: comparison.own_hash,
            computed: computed_own_hash,
        });
    }
    
    Ok(())
}
```

### 7. Bidirectional Sync

All protocols MUST be bidirectional to ensure convergence:

```rust
pub trait BidirectionalSync {
    /// Perform sync, returning actions for both sides
    fn sync(
        &self,
        channel: &mut NetworkChannel,
    ) -> Result<SyncResult>;
}

pub struct SyncResult {
    /// Actions to apply locally
    pub local_actions: Vec<Action>,
    
    /// Actions to send to peer for application
    pub remote_actions: Vec<Action>,
    
    /// Network statistics
    pub stats: NetworkStats,
}
```

### 8. Network Messages

```rust
pub enum SyncMessage {
    // Handshake
    Handshake(SyncHandshake),
    ProtocolSelected { protocol: SyncProtocol },
    ProtocolAck,
    ProtocolNack { reason: String },
    
    // Hash-based
    RequestEntities { ids: Vec<Id> },
    EntitiesResponse { entities: Vec<(Id, Vec<u8>, ComparisonData)> },
    
    // Snapshot
    RequestSnapshot { compressed: bool },
    SnapshotResponse { snapshot: Snapshot },
    
    // Bloom filter
    BloomFilter { filter: Vec<u8>, root_hash: [u8; 32] },
    BloomDiffResponse { missing: Vec<(Id, Vec<u8>, ComparisonData)> },
    
    // Bidirectional
    ActionsForPeer { actions: Vec<Action> },
    ActionsApplied { count: usize },
    
    // Verification
    VerificationFailed { reason: String },
    
    // Sync Hints (proactive sync triggers)
    DeltaWithHints { delta: CausalDelta, hints: SyncHints },
    StateAnnouncement { hints: SyncHints },
    RequestSyncMode { reason: SyncReason },
}
```

## Rationale

### Why Hybrid Approach?

1. **Delta sync (CmRDT)** is optimal for:
   - Real-time updates (low latency)
   - Small, incremental changes
   - Maintaining causal history

2. **State sync (CvRDT)** is optimal for:
   - Fresh node bootstrap
   - Large divergence recovery
   - Network partition healing

3. **Combining both** provides:
   - Best performance across all scenarios
   - Graceful degradation
   - Automatic recovery

### Why Negotiation?

Without negotiation, nodes might:
- Use incompatible protocols
- Choose suboptimal strategies
- Fail to sync due to capability mismatch

The handshake ensures both nodes agree on the best approach.

### Why Buffer Deltas?

During state-based sync:
- Applying deltas to partial state causes inconsistency
- Ignoring deltas loses data
- Buffering + replay ensures nothing is lost

### Why Bidirectional?

One-directional sync can't achieve root hash convergence when both nodes have unique data. Bidirectional ensures both nodes end up with identical state.

### Why Sync Hints in Delta Propagation?

Without hints, sync is **reactive**:
1. Node receives delta
2. Discovers missing parents
3. Requests parents recursively
4. Eventually times out or succeeds
5. Only then considers alternative sync

With hints, sync is **proactive**:
1. Node receives delta + hints
2. **Immediately** sees gap (delta_height, root_hash mismatch)
3. Makes informed decision: delta sync vs snapshot
4. No wasted round trips chasing deltas

**Key benefits:**
- **Faster recovery**: Fresh nodes don't waste time trying delta sync
- **Less bandwidth**: Avoid fetching 1000s of deltas only to give up
- **Better UX**: Users see "syncing snapshot" instead of hanging
- **Bloom filter efficiency**: O(1) membership test for delta existence

**Overhead is minimal:**
- Lightweight hints: 40 bytes (root_hash + delta_height)
- Full hints: ~200 bytes (with bloom filter)
- Compared to delta payload: Often 100+ bytes

## Backwards Compatibility

This CIP is backwards compatible:

1. **Existing delta sync** remains the default for nodes that don't support new protocols
2. **Handshake** allows capability discovery
3. **Fallback** to delta sync if negotiation fails

## Security Considerations

### 1. Malicious Snapshots

**Risk**: Peer sends tampered snapshot data.
**Mitigation**: Full cryptographic verification before applying.

### 2. Replay Attacks

**Risk**: Peer replays old deltas during sync.
**Mitigation**: HLC timestamps prevent accepting stale data.

### 3. Resource Exhaustion

**Risk**: Peer sends massive snapshot to exhaust memory.
**Mitigation**: Size limits, streaming, and compression.

### 4. Split-Brain

**Risk**: Network partition causes divergent states.
**Mitigation**: Deterministic conflict resolution (LWW, configurable per-entity).

## Test Cases

### Sync Protocol Tests (35 tests in `sync_protocol_negotiation.rs`)
1. âœ… **Protocol negotiation** - Full capability, mixed capability, version mismatch
2. âœ… **SyncHints** - Divergence detection, protocol suggestions, serialization
3. âœ… **DeltaBuffer** - FIFO order, overflow handling, reusability
4. âœ… **Adaptive selection** - No divergence, local empty, 10x difference, tree sizes

### Sync Integration Tests (14 tests in `sync_integration.rs`)
5. âœ… **Handshake negotiation** - Success and response construction
6. âœ… **Delta buffering** - During snapshot sync, overflow handling
7. âœ… **Full sync flow simulation** - End-to-end with multiple contexts
8. âœ… **Hints processing** - Entity count diff, tree depth, snapshot triggers

### Concurrent Merge Tests (17 tests in `concurrent_merge.rs`, 0.02s total)
9. âœ… **PureKvStore merge** - Disjoint keys, same key LWW, concurrent 10 keys each
10. âœ… **merge_root_state** - Injectable registry, global registry, serialization
11. âœ… **save_internal merge** - Older incoming timestamp, idempotent, same key LWW
12. âœ… **Unregistered type fallback** - Falls back to LWW correctly
13. âœ… **Real UnorderedMap merge** - 10 keys with actual CRDT types

### Hybrid Merge Tests (in `merge_integration.rs`)
14. âœ… **Counter merge** - Built-in CRDT, values sum correctly
15. âœ… **UnorderedMap per-key merge** - All keys preserved
16. âœ… **Vector merge** - Element-wise merge with LwwRegister
17. âœ… **UnorderedSet merge** - Add-wins union
18. âœ… **RGA merge** - Text converges
19. âœ… **Nested document merge** - Map of counters, map of LWW registers
20. âœ… **Custom type via callback** - `compare_trees_with_callback`
21. âœ… **Performance benchmark** - Built-in vs LWW comparison

### E2E Workflow Tests (4 workflows in `workflows/sync/`)
22. ğŸ”„ **crdt-merge.yml** - Two-node concurrent writes
23. ğŸ”„ **concurrent-sync.yml** - Delta buffering during sync
24. ğŸ”„ **three-node-convergence.yml** - 3-node network convergence
25. ğŸ”„ **late-joiner-large-state.yml** - Snapshot sync for large state gap

**Total: 66+ unit/integration tests passing, 4 E2E workflows ready**

## Implementation

### Phase 1: Storage Layer (COMPLETED)
- [x] `compare_trees_full` for Merkle comparison
- [x] `sync_trees` for recursive sync
- [x] `generate_snapshot` / `apply_snapshot` with verification
- [x] Resolution strategies (LWW, FirstWriteWins, MinValue, MaxValue)
- [x] Test protocols (HashBased, Snapshot, BloomFilter, SubtreePrefetch, LevelWise)

### Phase 2: Hybrid Merge Architecture âœ… DONE (Storage Layer)
> **This phase is critical** - without it, state sync loses CRDT data!

**2.1 Storage Layer Changes:** âœ…
- [x] Extend `CrdtType` enum with `Custom { type_name }` variant (all custom types MUST be Mergeable)
- [x] Add `crdt_type: Option<CrdtType>` field to `Metadata` struct
- [x] Collections auto-set crdt_type on creation:
  - [x] UnorderedMap â†’ `CrdtType::UnorderedMap`
  - [x] Vector â†’ `CrdtType::Vector`
  - [x] UnorderedSet â†’ `CrdtType::UnorderedSet`
  - N/A Counter, LwwRegister, RGA (inline CRDTs - merged via Mergeable trait, not Element-level)
- [x] Define `WasmMergeCallback` trait for custom type dispatch
- [x] Implement `merge_by_crdt_type_with_callback()` with hybrid dispatch logic
- [x] `compare_trees` uses CRDT-based merge (renamed from compare_trees_full)

**2.2 SDK/Macro Changes:** âœ…
- [x] `#[app::state]` macro enforces all fields are CRDT types or implement Mergeable
- [x] Compile error if non-CRDT scalar used without `LwwRegister<T>` wrapper
- N/A Root state doesn't need CrdtType::Custom (it's a container, fields handle their own types)

**2.3 Runtime Integration:** â†’ Moved to Phase 3.1
> Runtime integration requires networking context, moved to Phase 3.

**2.4 Tests:** âœ…
- [x] Built-in CRDT merge during state sync (Counter, Map) - merge_integration.rs
- [x] Custom type merge via callback (RegistryMergeCallback test)
- [x] Root state conflict triggers merge - merge_integration.rs
- [x] Compile error for non-CRDT field - apps updated with CRDT fields
- [x] Performance benchmark: built-in vs LWW merge - merge_integration.rs

**2.5 Cleanup:** âœ…
- [x] Removed `ResolutionStrategy` enum entirely (not deprecated, deleted)
- N/A merodb uses ABI for deserialization, doesn't need storage types

### Phase 3: Network Layer & Runtime Integration âœ… DONE

**3.1 Runtime Integration:** âœ…
- [x] `RuntimeMergeCallback` in `crates/runtime/src/merge_callback.rs`
- [x] `MockMergeCallback` for testing (custom handlers, call recording)
- [x] Falls back to type registry or LWW when WASM not available
- [ ] Wire up to `SyncManager` (deferred to Phase 4)

**3.2 Network Messages:** âœ…
- [x] `SyncHandshake` / `SyncHandshakeResponse` for protocol negotiation
- [x] `SyncCapabilities` for advertising supported protocols
- [x] `SyncProtocolVersion` enum (DeltaSync, SnapshotSync, HybridSync)
- [x] `SyncHints` in `BroadcastMessage::StateDelta` (~40 bytes overhead)
- [x] `SyncSessionState` for sync state machine
- [x] `DeltaBuffer` for buffering deltas during snapshot sync
- [x] `InitPayload::SyncHandshake` handler in `SyncManager`

**3.3 Tests:** âœ…
- [x] 9 sync_protocol unit tests (capabilities, hints, buffers, state)
- [x] 9 merge_callback unit tests (mock handlers, LWW, recording)
- [x] 27 integration tests (negotiation, scenarios, serialization)

### Phase 4: Integration âœ…
- [x] Wire `RuntimeMergeCallback` to `SyncManager` (`get_merge_callback()` ready for hash-based sync)
- [x] Delta buffering during state sync (`SyncSession` in `NodeState`)
- [x] Post-sync delta replay (triggers DAG sync for missing deltas)
- [x] Full sync state machine in `SyncManager` (`SyncSessionState` integration)
- [x] Proactive sync triggers based on hints (in `network_event.rs`)
- [x] Integration tests (14 tests in `sync_integration.rs`)
- [x] Periodic state announcements via `HashHeartbeat` (already exists, every 30s)
- [x] **Smart concurrent branch handling** in `ContextStorageApplier::apply()` (see Appendix G)
- [x] Parent hash tracking via `HashMap<delta_id, root_hash>`
- [x] Fixed LWW timestamp rejection in `save_internal()` for root entities
- [x] Concurrent merge unit tests (17 tests in `concurrent_merge.rs`, 0.02s execution)

**Note on Heartbeats vs SyncHints:**
- `HashHeartbeat` (30s interval): Lightweight divergence detection (`root_hash` + `dag_heads`)
- `SyncHints` (per delta): Rich metadata for protocol selection (`entity_count`, `tree_depth`)
- This split is intentional: heartbeats are high-frequency so kept minimal

**Key Insight: Concurrent Branch Detection**
The DAG model assumes linear delta application, but concurrent writes create divergent branches.
When node A applies a delta D2 from node B's concurrent branch:
- D2's `expected_root_hash` is based on B's state when D2 was created
- A's `current_root_hash` reflects A's divergent state  
- Simple hash comparison fails â†’ previously caused infinite sync loops

**Solution**: Smart merge detection (see Appendix G for algorithm)

### Phase 5: Optimization âœ…
- [x] Compressed snapshot transfer (lz4_flex, already implemented)
- [x] Streaming for large snapshots (pagination with resume_cursor)
- [x] Adaptive protocol selection (`SyncHints::adaptive_select()`)
- [x] Bloom filter (`DeltaIdBloomFilter`) for delta ID membership testing
- [x] Gossip mode selection (`GossipMode`: WithHints, Minimal, Adaptive)

### E2E Workflow Tests âœ…
- [x] `crdt-merge.yml` - Two-node concurrent writes + LWW conflict resolution
- [x] `concurrent-sync.yml` - Delta buffering during sync (500+100 keys)
- [x] `three-node-convergence.yml` - 3-node convergence (60 keys total)
- [x] `late-joiner-large-state.yml` - Snapshot sync for 2000-key state gap

### Phase 6: Delta Pruning (TODO)
- [ ] Checkpoint creation after snapshot sync
- [ ] Delta garbage collection protocol
- [ ] Tombstone cleanup mechanism

---

## Appendix A: Hybrid Merge Architecture

### Overview

The merge architecture has two categories of types:

1. **Built-in CRDTs**: Merge logic is deterministic and implemented in the storage layer
2. **Custom Mergeable Types**: Merge logic is defined in WASM by the application

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         State Sync                                  â”‚
â”‚                                                                     â”‚
â”‚  On conflict, check metadata.crdt_type:                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                               â”‚
              â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Built-in CRDTs            â”‚   â”‚   Custom Mergeable Types          â”‚
â”‚                             â”‚   â”‚                                   â”‚
â”‚   CrdtType::Counter         â”‚   â”‚   CrdtType::Custom {              â”‚
â”‚   CrdtType::UnorderedMap    â”‚   â”‚       type_name: "MyGameState",   â”‚
â”‚   CrdtType::Vector          â”‚   â”‚   }                               â”‚
â”‚   CrdtType::Rga             â”‚   â”‚                                   â”‚
â”‚   CrdtType::UnorderedSet    â”‚   â”‚                                   â”‚
â”‚   CrdtType::LwwRegister     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                             â”‚   â”‚   â”‚      WASM Module          â”‚   â”‚
â”‚   âœ… Merge in Storage Layer â”‚   â”‚   â”‚                           â”‚   â”‚
â”‚   âœ… No WASM needed         â”‚   â”‚   â”‚  impl Mergeable for       â”‚   â”‚
â”‚   âœ… ~100ns per merge       â”‚   â”‚   â”‚  MyGameState { ... }      â”‚   â”‚
â”‚                             â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚   â”‚                                   â”‚
â”‚                             â”‚   â”‚   âš ï¸ Requires WASM callback      â”‚
â”‚                             â”‚   â”‚   âš ï¸ ~10Î¼s per merge             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Problem: Type Information Not Stored

We already have the type system but don't store it with entities:

```rust
// âœ… HAVE: Type enumeration
pub enum CrdtType {
    LwwRegister, Counter, Rga, UnorderedMap, UnorderedSet, Vector,
    Custom { type_name: String }  // â† ONLY for app-defined #[app::state] types
}

// âœ… HAVE: Every CRDT knows its type
pub trait CrdtMeta {
    fn crdt_type() -> CrdtType;
}

// âœ… HAVE: Deterministic merge per built-in type
pub trait Mergeable {
    fn merge(&mut self, other: &Self) -> Result<(), MergeError>;
}

// âŒ MISSING: Type not stored with entity!
pub struct Metadata {
    pub created_at: u64,
    pub updated_at: UpdatedAt,
    pub storage_type: StorageType,
    pub resolution: ResolutionStrategy,  // â† Dumb (timestamp-based only)
    // WHERE IS crdt_type?!
}
```

### The Solution: Enhanced CrdtType Enum

```rust
/// CRDT type for merge dispatch
/// 
/// **All types in state MUST be mergeable!** Non-CRDT types break convergence.
/// Use `LwwRegister<T>` to wrap non-CRDT scalars (String, u64, etc.)
#[derive(BorshDeserialize, BorshSerialize, Clone, Debug)]
pub enum CrdtType {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // BUILT-IN TYPES: Storage layer merges directly (no WASM needed)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    /// G-Counter / PN-Counter: Sum per-node counts
    Counter,
    
    /// Last-Write-Wins Register: Higher timestamp wins
    /// Use this to wrap non-CRDT scalars: LwwRegister<String>, LwwRegister<u64>
    LwwRegister,
    
    /// Replicated Growable Array: Tombstone-based text CRDT
    Rga,
    
    /// Unordered Map: Per-key LWW or recursive merge
    UnorderedMap,
    
    /// Unordered Set: Add-wins union
    UnorderedSet,
    
    /// Vector: Element-wise merge
    Vector,
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CUSTOM TYPES: Requires WASM callback for merge
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    /// App-defined type with custom merge logic (MUST implement Mergeable)
    Custom {
        /// Type name for WASM dispatch (e.g., "MyGameState")
        type_name: String,
    },
}
```

### Updated Metadata Structure

```rust
#[derive(BorshDeserialize, BorshSerialize, Clone, Debug)]
pub struct Metadata {
    pub created_at: u64,
    pub updated_at: UpdatedAt,
    pub storage_type: StorageType,
    
    /// CRDT type for merge dispatch
    /// - Built-in types: Merged in storage layer
    /// - Custom types: May require WASM callback
    pub crdt_type: Option<CrdtType>,
    
    /// DEPRECATED: Use crdt_type instead
    /// Kept for backwards compatibility during migration
    #[deprecated(since = "0.5.0", note = "Use crdt_type for merge dispatch")]
    pub resolution: ResolutionStrategy,
}
```

### Merge Decision Table

| Type | Where Merged | WASM? | Performance | Example |
|------|--------------|-------|-------------|---------|
| Counter | Storage | âŒ No | ~100ns | `scores: Counter` |
| UnorderedMap | Storage | âŒ No | ~100ns | `items: UnorderedMap<K,V>` |
| Vector | Storage | âŒ No | ~100ns | `log: Vector<Event>` |
| Rga | Storage | âŒ No | ~100ns | `text: RGA` |
| UnorderedSet | Storage | âŒ No | ~100ns | `tags: UnorderedSet<String>` |
| LwwRegister | Storage | âŒ No | ~100ns | `name: LwwRegister<String>` |
| Custom | WASM | âœ… Yes | ~10Î¼s | `game: MyGameState` |
| Root State | WASM | âœ… Yes | ~10Î¼s | `#[app::state] MyApp` |
| Unknown (None) | Storage (LWW) | âŒ No | ~100ns | Legacy data only |

> âš ï¸ **All state types MUST be mergeable!** Non-CRDT scalars must be wrapped:
> - âŒ `name: String` â†’ âœ… `name: LwwRegister<String>`
> - âŒ `count: u64` â†’ âœ… `count: LwwRegister<u64>` or `count: Counter`

### WASM Merge Callback Interface

```rust
/// Trait for WASM merge callback - implemented by runtime
pub trait WasmMergeCallback: Send + Sync {
    /// Merge custom type via WASM
    ///
    /// # Arguments
    /// * `local` - Local entity data (Borsh-serialized)
    /// * `remote` - Remote entity data (Borsh-serialized)
    /// * `type_name` - Type name for dispatch (e.g., "MyGameState")
    ///
    /// # Returns
    /// Merged data (Borsh-serialized)
    fn merge(
        &self,
        local: &[u8],
        remote: &[u8],
        type_name: &str,
    ) -> Result<Vec<u8>, MergeError>;
    
    /// Merge root state (always custom)
    fn merge_root_state(
        &self,
        local: &[u8],
        remote: &[u8],
    ) -> Result<Vec<u8>, MergeError>;
}

/// Error types for merge operations
#[derive(Debug, Clone)]
pub enum MergeError {
    /// Built-in CRDT merge failed
    CrdtMergeError(String),
    
    /// WASM merge callback not provided for custom type
    WasmCallbackRequired { type_name: String },
    
    /// WASM merge function returned error
    WasmMergeError(String),
    
    /// Serialization/deserialization error
    SerializationError(String),
    
    /// Type mismatch during merge
    TypeMismatch { expected: String, found: String },
}
```

### Hybrid Merge Implementation

```rust
impl<S: StorageAdaptor> Interface<S> {
    /// Merge entity with hybrid dispatch
    pub fn merge_entity(
        local_data: &[u8],
        remote_data: &[u8],
        metadata: &Metadata,
        wasm_callback: Option<&dyn WasmMergeCallback>,
    ) -> Result<Vec<u8>, MergeError> {
        match &metadata.crdt_type {
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // BUILT-IN CRDTs: Merge directly in storage layer
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            Some(CrdtType::Counter) => {
                let mut local: Counter = borsh::from_slice(local_data)
                    .map_err(|e| MergeError::SerializationError(e.to_string()))?;
                let remote: Counter = borsh::from_slice(remote_data)
                    .map_err(|e| MergeError::SerializationError(e.to_string()))?;
                
                local.merge(&remote)
                    .map_err(|e| MergeError::CrdtMergeError(e.to_string()))?;
                
                borsh::to_vec(&local)
                    .map_err(|e| MergeError::SerializationError(e.to_string()))
            }
            
            Some(CrdtType::UnorderedMap) => {
                // Per-key merge with recursive CRDT support
                merge_unordered_map(local_data, remote_data, wasm_callback)
            }
            
            Some(CrdtType::Vector) => {
                merge_vector(local_data, remote_data, wasm_callback)
            }
            
            Some(CrdtType::Rga) => {
                let mut local: ReplicatedGrowableArray = borsh::from_slice(local_data)?;
                let remote: ReplicatedGrowableArray = borsh::from_slice(remote_data)?;
                local.merge(&remote)?;
                Ok(borsh::to_vec(&local)?)
            }
            
            Some(CrdtType::UnorderedSet) => {
                let mut local: UnorderedSet<_> = borsh::from_slice(local_data)?;
                let remote: UnorderedSet<_> = borsh::from_slice(remote_data)?;
                local.merge(&remote)?;
                Ok(borsh::to_vec(&local)?)
            }
            
            Some(CrdtType::LwwRegister) => {
                let mut local: LwwRegister<_> = borsh::from_slice(local_data)?;
                let remote: LwwRegister<_> = borsh::from_slice(remote_data)?;
                local.merge(&remote)?;
                Ok(borsh::to_vec(&local)?)
            }
            
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // CUSTOM TYPES: Dispatch to WASM
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // ONLY for user-defined #[app::state] types.
            // NOT for built-in wrappers like UserStorage/FrozenStorage
            // (those use their underlying collection's CrdtType).
            // All custom types MUST implement Mergeable in WASM.
            
            Some(CrdtType::Custom { type_name }) => {
                // App-defined type - MUST call WASM for merge
                let callback = wasm_callback.ok_or_else(|| {
                    MergeError::WasmCallbackRequired {
                        type_name: type_name.clone(),
                    }
                })?;
                
                callback.merge(local_data, remote_data, type_name)
            }
            
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // FALLBACK: No type info - use LWW
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            None => {
                // Legacy data or unknown type - LWW fallback
                lww_merge(local_data, remote_data, metadata)
            }
        }
    }
}

/// LWW merge fallback
fn lww_merge(
    local_data: &[u8],
    remote_data: &[u8],
    metadata: &Metadata,
) -> Result<Vec<u8>, MergeError> {
    // Compare timestamps - remote wins if newer or equal
    let local_ts = metadata.updated_at();
    // Assume remote timestamp is in the remote metadata
    // For now, remote wins on tie (consistent with existing behavior)
    Ok(remote_data.to_vec())
}
```

### Root State Merging

The root state (`#[app::state] struct MyApp`) is **always custom**:

```rust
#[app::state]
struct MyApp {
    // These are built-in CRDTs
    counter: Counter,
    map: UnorderedMap<String, String>,
    
    // This is a custom type
    game: MyGameState,
}

// The ROOT STRUCT itself is custom - needs WASM merge
impl Mergeable for MyApp {
    fn merge(&mut self, other: &Self) -> Result<(), MergeError> {
        // App defines how to merge the overall state
        self.counter.merge(&other.counter)?;
        self.map.merge(&other.map)?;
        self.game.merge(&other.game)?;  // Custom merge
        Ok(())
    }
}
```

When the ROOT entities conflict (same ID, different content), we MUST call WASM:

```rust
fn merge_root_state(
    local: &[u8],
    remote: &[u8],
    wasm_callback: &dyn WasmMergeCallback,
) -> Result<Vec<u8>, MergeError> {
    // Root state is always custom - must use WASM
    wasm_callback.merge_root_state(local, remote)
}
```

### Collections Auto-Set Type

```rust
// Counter sets its type on creation
impl<S: StorageAdaptor> Counter<S> {
    pub fn new() -> Self {
        let mut element = Element::new();
        element.metadata_mut().crdt_type = Some(CrdtType::Counter);
        Self { element, counts: BTreeMap::new() }
    }
}

// UnorderedMap sets its type on creation
impl<K, V, S: StorageAdaptor> UnorderedMap<K, V, S> {
    pub fn new() -> Self {
        let mut element = Element::new();
        element.metadata_mut().crdt_type = Some(CrdtType::UnorderedMap);
        Self { element, entries: BTreeMap::new(), _phantom: PhantomData }
    }
}

// Custom types set via macro
#[app::state]  // Macro generates:
struct MyApp { /*...*/ }
// element.metadata_mut().crdt_type = Some(CrdtType::Custom {
//     type_name: "MyApp".to_string(),
// });
```

### Enforcing CRDT-Only State (Compile-Time)

The `#[app::state]` macro MUST reject non-CRDT fields:

```rust
// âœ… VALID: All fields are CRDTs
#[app::state]
struct MyApp {
    scores: Counter,                        // Built-in CRDT
    items: UnorderedMap<String, String>,    // Built-in CRDT
    name: LwwRegister<String>,              // Wrapped scalar
    config: LwwRegister<MyConfig>,          // Wrapped custom type
    game: MyGameState,                      // Custom Mergeable
}

// âŒ COMPILE ERROR: Raw scalars not allowed
#[app::state]
struct BadApp {
    name: String,        // ERROR: Use LwwRegister<String>
    count: u64,          // ERROR: Use LwwRegister<u64> or Counter
    data: Vec<u8>,       // ERROR: Use Vector<u8>
}
```

**Macro enforcement logic:**
```rust
// In #[app::state] macro
fn validate_field_type(ty: &Type) -> Result<(), CompileError> {
    if is_builtin_crdt(ty) {
        Ok(())  // Counter, UnorderedMap, Vector, etc.
    } else if is_lww_register(ty) {
        Ok(())  // LwwRegister<T> wraps any type
    } else if implements_mergeable(ty) {
        Ok(())  // Custom Mergeable type
    } else {
        Err(CompileError::new(
            format!(
                "Field type `{}` is not a CRDT. Wrap with LwwRegister<{}> or implement Mergeable.",
                ty, ty
            )
        ))
    }
}
```

This ensures **all state converges** - no silent data loss from LWW on non-CRDT types.

### The Generic Type Problem - SOLVED

**Question**: For `UnorderedMap<K, V>`, don't we need to know K and V types?

**Answer**: NO! Each entity stores its own `crdt_type` in Metadata.

```
UnorderedMap<String, Counter> in storage:
â”‚
â”œâ”€â”€ Map Entity (id: 0x123)
â”‚   â””â”€â”€ metadata.crdt_type = Some(CrdtType::UnorderedMap)
â”‚
â”œâ”€â”€ Entry "alice" (id: 0x456, parent: 0x123)
â”‚   â””â”€â”€ metadata.crdt_type = Some(CrdtType::Counter)  â† Self-describing!
â”‚
â””â”€â”€ Entry "bob" (id: 0x789, parent: 0x123)
    â””â”€â”€ metadata.crdt_type = Some(CrdtType::Counter)  â† Self-describing!
```

**Merge algorithm**:
```rust
fn merge_entity(local: &Entity, remote: &Entity) -> Result<Vec<u8>> {
    // Each entity knows its own type - no ABI needed!
    match &local.metadata.crdt_type {
        Some(CrdtType::UnorderedMap) => {
            // Merge map: iterate children, merge each by THEIR crdt_type
            for (local_child, remote_child) in children_pairs {
                merge_entity(local_child, remote_child)?;  // Recursive!
            }
        }
        Some(CrdtType::Counter) => {
            // Merge counter directly
            let mut local: Counter = deserialize(local.data)?;
            let remote: Counter = deserialize(remote.data)?;
            local.merge(&remote)?;
        }
        // ...
    }
}
```

**No ABI required!** The Merkle tree is self-describing - every entity carries its type.

### Performance Analysis

```
Merge Benchmark (1000 entities):

Built-in CRDTs (Counter, Map, etc.):
â”œâ”€â”€ Conflicts: 100 entities
â”œâ”€â”€ Merge time: 100 Ã— 100ns = 10Î¼s total
â””â”€â”€ WASM calls: 0

Custom Mergeable Types:
â”œâ”€â”€ Conflicts: 10 entities
â”œâ”€â”€ Merge time: 10 Ã— 10Î¼s = 100Î¼s total
â””â”€â”€ WASM calls: 10

Root State Conflicts:
â”œâ”€â”€ Conflicts: 1 (rare - only on concurrent root updates)
â”œâ”€â”€ Merge time: 1 Ã— 10Î¼s = 10Î¼s
â””â”€â”€ WASM calls: 1

Total: ~120Î¼s for 111 conflicts
Network RTT: ~50ms

Merge overhead: 0.24% of sync time
```

### Sync API with WASM Callback

```rust
impl SyncManager {
    /// Sync state with hybrid merge support
    pub async fn sync_with_peer(&self, peer: PeerId) -> Result<SyncResult> {
        let foreign_state = self.fetch_state(peer).await?;
        
        // Create WASM callback if we have a loaded module
        let wasm_callback: Option<Box<dyn WasmMergeCallback>> = 
            self.wasm_module.as_ref().map(|m| {
                Box::new(WasmMergeCallbackImpl::new(m)) as Box<dyn WasmMergeCallback>
            });
        
        // Compare trees with hybrid merge
        let (local_actions, remote_actions) = Interface::compare_trees_full_with_merge(
            self.root_id,
            &foreign_state.index,
            &foreign_state.data,
            wasm_callback.as_deref(),
        )?;
        
        // Apply merged actions
        for action in local_actions {
            Interface::apply_action(&action)?;
        }
        
        // Send remote's needed actions
        self.send_actions(peer, remote_actions).await?;
        
        Ok(SyncResult::Completed)
    }
}
```

### Migration Path

| Phase | Change | Backwards Compatible? |
|-------|--------|----------------------|
| 1 | Add `crdt_type: Option<CrdtType>` to Metadata | âœ… Yes (Optional field) |
| 2 | Collections auto-set crdt_type on creation | âœ… Yes (Additive) |
| 3 | `#[app::state]` macro sets Custom type | âœ… Yes (Additive) |
| 4 | `compare_trees_full` uses crdt_type for dispatch | âœ… Yes |
| 5 | Add WasmMergeCallback trait | âœ… Yes (Optional) |
| 6 | SyncManager creates callback from WASM module | âœ… Yes |
| 7 | Deprecate ResolutionStrategy | âš ï¸ Migration needed |

**Note**: No ABI required! Each entity stores its own `crdt_type` in Metadata - the tree is self-describing.

### Summary: Why This Architecture

| Aspect | Old (ResolutionStrategy) | New (Hybrid CrdtType) |
|--------|--------------------------|----------------------|
| Built-in CRDT merge | âŒ LWW only (data loss!) | âœ… Proper CRDT merge |
| Custom type merge | âŒ Not supported | âœ… Via WASM callback |
| Performance | N/A | ~100ns built-in, ~10Î¼s custom |
| WASM dependency | Required for all | Only for custom types |
| Type safety | None | Compile-time for built-in |
| Extensibility | None | App can define merge logic |

---

## Appendix B: Protocol Selection Matrix

### When to Use Each Protocol

| Protocol | Trigger Conditions | Best For | Avoid When |
|----------|-------------------|----------|------------|
| **DeltaSync** | Missing < 10 deltas, parents known | Real-time updates, small gaps | Fresh nodes, large gaps |
| **HashBasedSync** | Divergence 10-50%, depth any | General-purpose catch-up | 100% divergence (fresh node) |
| **BloomFilterSync** | Entities > 50, divergence < 10% | Large trees with tiny diff | Small trees, high divergence |
| **SubtreePrefetchSync** | Depth > 3, divergence < 20% | Deep hierarchies, localized changes | Shallow trees, scattered changes |
| **LevelWiseSync** | Depth â‰¤ 2 | Wide shallow trees | Deep hierarchies |
| **SnapshotSync** | Fresh node OR divergence > 50% | Bootstrap, major divergence | Tiny diffs (wasteful) |
| **CompressedSnapshotSync** | Entities > 100, fresh node | Large state bootstrap | Small state, low bandwidth |

### Protocol Selection Flowchart

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Start Sync Decision â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ root_hash matches?  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        Yes â”‚      â”‚ No
                            â”‚      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”   â”‚
                    â”‚ NO SYNC  â”‚   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                               â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚ Has local state?  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                No â”‚       â”‚ Yes
                                   â”‚       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”   â”‚
                        â”‚ SNAPSHOT     â”‚   â”‚
                        â”‚ (compressed  â”‚   â”‚
                        â”‚  if >100)    â”‚   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚ Estimate          â”‚
                                       â”‚ divergence ratio  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                          â”‚                          â”‚
              >50%  â”‚                    10-50%â”‚                     <10% â”‚
                    â”‚                          â”‚                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ SNAPSHOT        â”‚      â”‚ Check tree shape    â”‚    â”‚ BLOOM_FILTER      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (if entities >50) â”‚
                                               â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚                â”‚                â”‚
                        depth>3         depthâ‰¤2          default
                              â”‚                â”‚                â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ SUBTREE_PREFETCHâ”‚ â”‚ LEVEL_WISEâ”‚ â”‚ HASH_BASED      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Appendix B: Eventual Consistency Guarantees

### How We Ensure All Nodes Converge

#### 1. Merkle Root Hash Invariant

**Guarantee**: After successful bidirectional sync, `root_hash(A) == root_hash(B)`

```
Before Sync:                 After Sync:
  Node A: [hash_a]             Node A: [hash_final]
  Node B: [hash_b]             Node B: [hash_final]
  
  hash_a â‰  hash_b              hash_final == hash_final âœ“
```

#### 2. Multi-Node Convergence (Gossip)

With N > 2 nodes, pairwise sync eventually converges:

```
Time T0:
  A: [h1]  B: [h2]  C: [h3]  (all different)

Time T1: A syncs with B
  A: [h12] B: [h12] C: [h3]

Time T2: B syncs with C  
  A: [h12] B: [h123] C: [h123]

Time T3: A syncs with B (or C)
  A: [h123] B: [h123] C: [h123]  âœ“ All converged
```

**Convergence Bound**: O(log N) sync rounds with random pairwise selection.

#### 3. Conflict Resolution Determinism

Same inputs â†’ Same output (deterministic merge):

```rust
// Given same conflict data, all nodes make same decision
let result_a = resolve_conflict(local_data, foreign_data, strategy);
let result_b = resolve_conflict(local_data, foreign_data, strategy);
assert_eq!(result_a, result_b);  // Always true
```

**Strategies and their determinism:**

| Strategy | Deterministic? | Tie-breaker |
|----------|---------------|-------------|
| LastWriteWins | âœ… Yes | HLC timestamp, then data bytes |
| FirstWriteWins | âœ… Yes | HLC timestamp |
| MaxValue | âœ… Yes | Byte comparison |
| MinValue | âœ… Yes | Byte comparison |
| Manual | âš ï¸ Requires app logic | App-defined |

#### 4. Causal Consistency via DAG

Deltas are applied in causal order:

```
Delta D3 (parents: [D1, D2])
    â†“
Cannot apply D3 until D1 AND D2 are applied
    â†“
Guarantees causal consistency
```

---

## Appendix C: Delta Pruning

### The Problem

Without pruning, delta history grows forever:
- Genesis â†’ Delta1 â†’ Delta2 â†’ ... â†’ Delta1000000
- New nodes must process ALL deltas (inefficient)
- Storage grows unbounded

### The Solution: Checkpoints

```rust
pub struct Checkpoint {
    /// Unique checkpoint ID
    pub id: [u8; 32],
    
    /// Root hash at checkpoint time
    pub root_hash: [u8; 32],
    
    /// HLC timestamp when created
    pub timestamp: HybridTimestamp,
    
    /// Full state snapshot
    pub snapshot: Snapshot,
    
    /// Last delta ID included in this checkpoint
    pub last_delta_id: [u8; 32],
    
    /// Signatures from N/M nodes (quorum attestation)
    pub attestations: Vec<NodeAttestation>,
}
```

### Checkpoint Creation Protocol

```
1. Leader proposes checkpoint at delta height H
2. Nodes verify their state matches proposed root_hash
3. Nodes sign attestation if state matches
4. Once quorum (e.g., 2/3) attestations collected:
   - Checkpoint is finalized
   - Deltas before H can be pruned
5. New nodes can start from checkpoint instead of genesis
```

### Pruning Safety

**Critical Invariant**: Only prune deltas if:
1. Checkpoint exists with root_hash matching current state
2. Quorum of nodes attested to the checkpoint
3. All nodes have received the checkpoint

```rust
impl CheckpointStore {
    fn can_prune_delta(&self, delta: &CausalDelta, checkpoint: &Checkpoint) -> bool {
        // Delta is before checkpoint
        delta.hlc < checkpoint.timestamp
            // AND checkpoint is finalized
            && checkpoint.attestations.len() >= QUORUM_SIZE
            // AND we have the checkpoint snapshot
            && self.has_checkpoint(&checkpoint.id)
    }
}
```

### Relationship with State Sync

| Scenario | Bootstrap From |
|----------|---------------|
| Has checkpoint | Checkpoint snapshot + deltas after checkpoint |
| No checkpoint | Genesis + all deltas OR peer snapshot |

---

## Appendix D: Edge Cases & Missing Pieces

### Edge Case 1: Concurrent Sync + Modifications âœ… SOLVED

**Problem**: Node A is syncing from B while C sends new deltas.

**Solution**: Delta buffering (implemented in Phase 4)

```
During Sync:
  [Incoming deltas] â†’ Buffer (via SyncSession in NodeState)
  [Sync state] â†’ Apply directly
  
After Sync:
  [Buffer] â†’ Trigger DAG sync â†’ Apply missing deltas
```

**Implementation**: `NodeState::start_sync_session()`, `buffer_delta()`, `end_sync_session()`

### Edge Case 1b: Concurrent Writes Creating Divergent Branches âœ… SOLVED

**Problem**: Two nodes apply deltas concurrently, creating branches. When deltas propagate:
- D2a expects hash based on Node A's state
- D2b expects hash based on Node B's state  
- Applying D2b on Node A fails: `RootHashMismatch`

**Solution**: Smart concurrent branch detection (Appendix G)

```rust
// Detect merge scenario
let is_merge = current_root != delta.expected_root 
    && parent_hash != Some(current_root);

if is_merge {
    // Use CRDT merge instead of direct apply
    sync_trees_with_callback(actions, merge_callback);
}
```

**Implementation**: `ContextStorageApplier::apply()` in `delta_store.rs`

### Edge Case 2: Partial Sync Failure

**Problem**: Sync fails midway (network error, node crash).

**Solution**: Atomic sync with rollback

```rust
pub struct SyncTransaction {
    /// Original state before sync started
    rollback_snapshot: Snapshot,
    
    /// Partial state received so far
    partial_state: PartialState,
    
    /// Has sync completed successfully?
    committed: bool,
}

impl Drop for SyncTransaction {
    fn drop(&mut self) {
        if !self.committed {
            // Rollback to original state
            apply_snapshot_unchecked(&self.rollback_snapshot);
        }
    }
}
```

### Edge Case 3: Byzantine/Malicious Nodes

**Problem**: Node sends tampered data.

**Solution**: Cryptographic verification (already implemented)

| Attack | Defense |
|--------|---------|
| Tampered entity data | Hash verification fails |
| Tampered root hash | Computed root â‰  claimed root |
| Replay old snapshot | HLC timestamp check |
| Forge attestations | Signature verification |

### Edge Case 4: Clock Skew

**Problem**: Node clocks are significantly different.

**Solution**: HLC bounds + peer clock sync

```rust
const MAX_CLOCK_SKEW: Duration = Duration::from_secs(60);

fn validate_delta_timestamp(delta: &CausalDelta, local_hlc: &HybridTimestamp) -> bool {
    let drift = delta.hlc.physical_diff(local_hlc);
    drift < MAX_CLOCK_SKEW
}
```

### Edge Case 5: Large Entities

**Problem**: Single entity is huge (e.g., 100MB blob).

**Solution**: Chunked transfer with streaming

```rust
pub enum SyncMessage {
    // ... existing messages ...
    
    /// Large entity transferred in chunks
    EntityChunk {
        id: Id,
        chunk_index: u32,
        total_chunks: u32,
        data: Vec<u8>,
        chunk_hash: [u8; 32],
    },
}
```

### Edge Case 6: Tombstone Accumulation

**Problem**: Deleted entities leave tombstones forever.

**Solution**: Tombstone TTL + garbage collection

```rust
pub struct Tombstone {
    pub deleted_at: HybridTimestamp,
    pub ttl: Duration,  // e.g., 30 days
}

fn should_gc_tombstone(tombstone: &Tombstone, now: HybridTimestamp) -> bool {
    now.physical_time() > tombstone.deleted_at.physical_time() + tombstone.ttl
}
```

**GC Safety**: Only GC tombstones after:
1. TTL expired
2. All active nodes have seen the deletion
3. Checkpoint created after deletion

### Edge Case 7: Network Partition Healing

**Problem**: Two partitions evolved independently, now reconnecting.

```
Partition 1: A, B â†’ root_hash_1 (1000 entities)
Partition 2: C, D â†’ root_hash_2 (1000 entities)

After heal: 4 nodes, 2 different states
```

**Solution**: Merge reconciliation protocol

```rust
fn heal_partition(
    partition1_root: [u8; 32],
    partition2_root: [u8; 32],
) -> HealingStrategy {
    // Compare entity counts
    let p1_count = get_entity_count(partition1_root);
    let p2_count = get_entity_count(partition2_root);
    
    // If one partition has significantly more state, it likely has more truth
    // But we still need bidirectional merge
    
    HealingStrategy::BidirectionalMerge {
        // Sync partition1 â†’ partition2
        // Then sync partition2 â†’ partition1 (updated)
        // Repeat until convergence
    }
}
```

### Edge Case 8: Schema Evolution

**Problem**: Entity format changes between versions.

**Solution**: Version tagging + migration

```rust
pub struct EntityEnvelope {
    pub version: u32,
    pub data: Vec<u8>,
}

fn deserialize_entity(envelope: &EntityEnvelope) -> Result<Entity> {
    match envelope.version {
        1 => deserialize_v1(&envelope.data),
        2 => deserialize_v2(&envelope.data),
        v => Err(UnknownVersion(v)),
    }
}
```

---

## Appendix E: What's Still Missing

### Critical Gaps

| Gap | Severity | Status |
|-----|----------|--------|
| **CrdtType not stored in Metadata** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 2) - `crdt_type: Option<CrdtType>` in Metadata |
| **No WasmMergeCallback for custom types** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 3) - `WasmMergeCallback` trait + `RuntimeMergeCallback` |
| **Concurrent branch merge failures** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 4) - Smart merge detection in `delta_store.rs` |
| **LWW rejecting root merges** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 4) - Root entities always attempt CRDT merge first |
| **Collection IDs are random, not deterministic** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 5) - Deterministic IDs via `new_with_field_name()` |
| **Hash mismatch rejecting valid deltas** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 5) - Trust CRDT semantics, see Appendix I |
| **parent_hashes storing wrong value** | ğŸ”´ CRITICAL | âœ… FIXED (Phase 5) - Store computed hash, not expected hash |
| **merodb duplicates types (out of sync)** | ğŸŸ¡ HIGH | TODO - See Appendix F for fix plan |
| **Checkpoint protocol not implemented** | ğŸŸ¡ HIGH | TODO (Phase 6) - Nodes keep all deltas forever |
| **No quorum-based attestation** | ğŸŸ¡ HIGH | TODO - Single malicious node could create fake checkpoint |
| **Tombstone GC not implemented** | ğŸŸ  MEDIUM | TODO - Deleted entities accumulate |
| **Large entity streaming** | ğŸŸ  MEDIUM | Partial - Pagination exists, chunked transfer TODO |
| **Partition healing protocol** | ğŸŸ  MEDIUM | Partial - Bidirectional sync helps, explicit protocol TODO |

### Nice-to-Have Improvements

| Improvement | Benefit |
|-------------|---------|
| Merkle proof for single entity sync | Verify entity without full state |
| Incremental checkpoint updates | Don't regenerate full snapshot |
| Probabilistic sync skip | Skip sync if bloom filter shows no diff |
| Adaptive sync frequency | Sync more often during high activity |

### Open Questions

1. **Checkpoint Frequency**: How often should checkpoints be created?
   - Too frequent: High storage/network cost
   - Too rare: Long bootstrap times
   - Proposal: Every 1000 deltas OR 1 hour, whichever first

2. **Quorum Size**: What's the right attestation quorum?
   - 2/3 + 1 (Byzantine fault tolerant)
   - Simple majority (crash fault tolerant only)
   - Proposal: Configurable per context

3. **Tombstone TTL**: How long to keep tombstones?
   - Too short: Resurrection attacks possible
   - Too long: Storage bloat
   - Proposal: 30 days default, configurable

4. **Cross-Context Sync**: Can contexts share sync infrastructure?
   - Separate sync per context (current)
   - Shared sync layer with context isolation
   - Proposal: Keep separate for security

---

## Appendix F: merodb Type Synchronization

### The Problem: Duplicated Types

merodb duplicates storage types instead of importing them:

```rust
// tools/merodb/src/export.rs (DUPLICATE - OUT OF SYNC!)
struct Metadata {
    created_at: u64,
    updated_at: UpdatedAt,
    storage_type: StorageType,
    // MISSING: resolution â† DESERIALIZATION FAILS!
}

// calimero-storage/src/entities.rs (ACTUAL)
pub struct Metadata {
    pub created_at: u64,
    pub updated_at: UpdatedAt,
    pub storage_type: StorageType,
    pub resolution: ResolutionStrategy,  // â† EXISTS
}
```

This means **merodb cannot deserialize current storage data**.

### The Fix: Import from calimero-storage

```rust
// tools/merodb/src/export.rs - BEFORE (BROKEN)
#[derive(borsh::BorshDeserialize)]
struct EntityIndex {
    id: Id,
    parent_id: Option<Id>,
    children: Option<Vec<ChildInfo>>,
    full_hash: [u8; 32],
    own_hash: [u8; 32],
    metadata: Metadata,  // â† Local duplicate
    deleted_at: Option<u64>,
}

// tools/merodb/src/export.rs - AFTER (FIXED)
use calimero_storage::index::EntityIndex;
use calimero_storage::entities::{Metadata, ChildInfo, Element};
use calimero_storage::address::Id;
// Remove all local struct definitions
```

### Why This Matters for the New Architecture

When we add `crdt_type` to `Metadata`:

```rust
// calimero-storage/src/entities.rs (NEW)
pub struct Metadata {
    pub created_at: u64,
    pub updated_at: UpdatedAt,
    pub storage_type: StorageType,
    pub resolution: ResolutionStrategy,
    pub crdt_type: Option<CrdtType>,  // â† NEW
}
```

merodb will:
1. **If importing from calimero-storage**: Automatically get the new field âœ…
2. **If duplicating**: Fail to deserialize (again) âŒ

### Full Refactoring Plan for merodb

| File | Current | Fix |
|------|---------|-----|
| `export.rs` | Duplicates `EntityIndex`, `Metadata`, `Id`, `ChildInfo` | Import from `calimero_storage` |
| `types.rs` | Some duplicates | Import from `calimero_store` |
| `deserializer.rs` | Uses ABI (correct) | No change needed |

### Code Changes Required

```rust
// tools/merodb/src/export.rs

// ADD these imports
use calimero_storage::{
    index::EntityIndex,
    entities::{Metadata, ChildInfo, Element, StorageType, ResolutionStrategy},
    address::Id,
};

// REMOVE these local definitions (lines ~938-1470)
// - struct EntityIndex
// - struct Id  
// - struct ChildInfo
// - struct Metadata
// - struct UpdatedAt
// - enum StorageType
// - struct SignatureData
```

### Benefits of Import Over Duplication

| Aspect | Duplication | Import |
|--------|-------------|--------|
| Maintenance | Manual sync required | Automatic |
| Breaking changes | Silent failure | Compile error |
| Type safety | None | Full |
| Schema evolution | Breaks | Works |

### Migration Steps

1. **Phase 1**: Update merodb to import from `calimero_storage`
   - Remove duplicated structs
   - Fix any API differences
   - Test with current storage format

2. **Phase 2**: Add `crdt_type` to `Metadata`
   - Storage and merodb update together
   - merodb automatically gets the new field

3. **Phase 3**: merodb uses `crdt_type` for smart deserialization
   - Instead of relying solely on ABI
   - Can deserialize without WASM/ABI if type is stored

### Example: Smart Deserialization with CrdtType

```rust
fn decode_state_entry(bytes: &[u8], manifest: &Manifest) -> Option<Value> {
    // First try as EntityIndex
    if let Ok(index) = borsh::from_slice::<EntityIndex>(bytes) {
        // NEW: Check crdt_type for smart dispatch
        if let Some(crdt_type) = &index.metadata.crdt_type {
            return match crdt_type {
                CrdtType::Counter => decode_counter(bytes),
                CrdtType::UnorderedMap => decode_map(bytes, manifest),
                CrdtType::Rga => decode_rga(bytes),
                _ => None,
            };
        }
        
        // Fallback to ABI-based deserialization
        return decode_with_abi(bytes, manifest);
    }
    None
}
```

This enables merodb to:
1. Work without ABI (if crdt_type is set)
2. Display CRDT-specific UI (show counter value, map entries, etc.)
3. Support merge visualization (show how CRDTs would merge)

---

## Appendix G: Smart Concurrent Branch Handling

### The Problem: DAG vs Concurrent Writes

The DAG model assumes **linear delta application**:
```
Genesis â†’ D1 â†’ D2 â†’ D3 â†’ ...
           â†‘
           Each delta's expected_root_hash = previous delta's result
```

But **concurrent writes** create **divergent branches**:
```
              â”Œâ”€â”€â”€ D2a (Node A) â”€â”€â”€â”
              â”‚                    â”‚
Genesis â†’ D1 â”€â”¤                    â”œâ”€â”€> ???
              â”‚                    â”‚
              â””â”€â”€â”€ D2b (Node B) â”€â”€â”€â”˜
```

When Node A receives D2b from Node B:
- D2b's `expected_root_hash` = B's state after D1 (before D2a)
- A's `current_root_hash` = A's state after D2a
- **Mismatch!** â†’ Old behavior: `RootHashMismatch` error â†’ sync loop

### The Solution: Merge Scenario Detection

**Key insight**: We can detect merge scenarios by tracking parent hashes.

```rust
// In ContextStorageApplier::apply()
let is_merge_scenario = 
    current_root_hash != delta.expected_root_hash     // Hashes don't match
    && parent_root_hash != Some(current_root_hash);   // Parent isn't current state
```

**Decision matrix:**

| `current == expected` | `parent == current` | Scenario | Action |
|----------------------|---------------------|----------|--------|
| âœ… Yes | N/A | Linear application | Apply normally |
| âŒ No | âœ… Yes | Already diverged | `RootHashMismatch` error |
| âŒ No | âŒ No | **Concurrent branch** | **CRDT merge** |

### The Algorithm

```rust
impl ContextStorageApplier {
    async fn apply(&self, delta: CausalDelta) -> Result<(), ApplyError> {
        // 1. Get current state
        let current_root_hash = self.context_client.get_context(&self.context_id)?.root_hash;
        
        // 2. Look up parent's root hash (tracked after each delta application)
        let parent_root_hash = if delta.parents.len() == 1 && delta.parents[0] != [0u8; 32] {
            self.parent_hashes.read().await.get(&delta.parents[0]).copied()
        } else {
            None
        };
        
        // 3. Detect merge scenario
        let is_merge = current_root_hash != delta.expected_root_hash
            && parent_root_hash.map_or(true, |p| p != current_root_hash);
        
        // 4. Apply with appropriate strategy
        let outcome = if is_merge {
            // MERGE: Use sync_trees_with_callback for CRDT semantics
            info!("Concurrent branch detected - applying with CRDT merge");
            let callback = Arc::new(RuntimeMergeCallback::new());
            self.context_client.sync_trees_with_callback(
                &self.context_id,
                &self.our_identity,
                delta.payload.clone(),
                Some(callback),
            ).await?
        } else {
            // NORMAL: Direct WASM execution
            self.context_client.execute(
                &self.context_id,
                &self.our_identity,
                "__calimero_sync_next",
                artifact,
                vec![],
                None,
            ).await?
        };
        
        // 5. Verify root hash (skip for merge - new hash expected)
        if !is_merge && outcome.root_hash != delta.expected_root_hash {
            return Err(ApplyError::RootHashMismatch { 
                computed: outcome.root_hash, 
                expected: delta.expected_root_hash 
            });
        }
        
        // 6. Track this delta's result for future merge detection
        self.parent_hashes.write().await.insert(delta.id, *outcome.root_hash);
        
        Ok(())
    }
}
```

### Parent Hash Tracking

```rust
pub struct ContextStorageApplier {
    // ... existing fields ...
    
    /// Maps delta_id -> root_hash after that delta was applied
    /// Used to detect concurrent branches vs linear history
    parent_hashes: Arc<RwLock<HashMap<[u8; 32], [u8; 32]>>>,
}
```

**Populated from:**
1. **On delta application**: After each successful apply, store `delta.id -> result_root_hash`
2. **On startup**: Load from persisted deltas in DAG store

### LWW Timestamp Fix in `save_internal`

**Previous bug**: Root entity updates with older timestamps were rejected before CRDT merge could happen.

```rust
// BEFORE (BUGGY):
if last_metadata.updated_at > metadata.updated_at {
    return Ok(None);  // REJECT - but this skips CRDT merge!
}
// ... then do merge ...

// AFTER (FIXED):
if id.is_root() {
    // Root entity - ALWAYS attempt merge first
    if let Some(existing_data) = S::storage_read(Key::Entry(id)) {
        let merged = try_merge_data(id, &existing_data, data, ...)?;
        // Merge handles timestamps internally via CRDT semantics
    }
} else if last_metadata.updated_at > metadata.updated_at {
    return Ok(None);  // LWW for non-root entities
}
```

**Why this matters:**
- Concurrent writes often have "older" timestamps from the sender's perspective
- Root state contains nested CRDTs (Counter, Map, etc.) that MUST merge
- Rejecting based on root timestamp loses CRDT data

### Visual: Linear vs Concurrent

```
LINEAR APPLICATION (no merge needed):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ hash_0  â”‚â”€â”€D1â”€â”€â”‚ hash_1  â”‚â”€â”€D2â”€â”€â”‚ hash_2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†‘                â†‘
                   D2.parent        D2.expected
                   == hash_1        == hash_2    âœ… Match

CONCURRENT BRANCHES (merge needed):
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”Œâ”€D2aâ”‚ hash_2a â”‚  (Node A)
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ hash_1  â”‚â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â””â”€D2bâ”‚ hash_2b â”‚  (Node B)
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†‘
                 D2b.expected == hash_2b
                 
When A receives D2b:
  current_root = hash_2a
  D2b.expected = hash_2b
  parent_of(D2b) = hash_1 â‰  hash_2a
  
  â†’ Merge scenario detected!
  â†’ Apply D2b actions with CRDT merge
  â†’ Result: hash_merged (combines both branches)
```

### Test Coverage

| Test | What it verifies |
|------|------------------|
| `test_pure_kv_merge_disjoint_keys` | Disjoint keys from two nodes merge correctly |
| `test_pure_kv_merge_same_key_lww` | Same key conflict uses LWW per inner timestamps |
| `test_merge_root_older_incoming_timestamp` | Older root timestamp doesn't reject merge |
| `test_merge_idempotent` | Merging same state twice is no-op |
| `test_concurrent_10_keys_each_via_merge_root_state` | 20 keys from 2 nodes all preserved |
| `test_try_merge_data_delegates_correctly` | Storage layer delegates to registry |
| `test_real_unordered_map_merge` | Actual UnorderedMap CRDT merges correctly |

**All 17 concurrent merge tests pass in 0.02s** (no storage layer overhead).

### Error Handling: RootHashMismatch

When a true `RootHashMismatch` occurs (not a merge scenario):

```rust
// In handle_state_delta()
match delta_store_ref.add_delta_with_events(delta, events).await {
    Err(e) if e.to_string().contains("RootHashMismatch") => {
        warn!("Divergent histories detected - triggering state sync");
        
        // Trigger full state sync to reconcile
        node_clients.node.sync(Some(&context_id), Some(&source)).await?;
        
        // Return error - delta will be retried after sync
        return Err(e);
    }
    // ... other error handling
}
```

This ensures:
1. Divergence is detected and logged
2. State sync is triggered automatically
3. Delta is retried after sync completes (not lost)

## References

- [CRDT Literature](https://crdt.tech/)
- [Merkle Trees](https://en.wikipedia.org/wiki/Merkle_tree)
- [Hybrid Logical Clocks](https://cse.buffalo.edu/tech-reports/2014-04.pdf)
- [EIP-1 Format](https://eips.ethereum.org/EIPS/eip-1)

## Appendix H: Collection ID Randomization Bug (âœ… FIXED)

### The Problem: Non-Deterministic Collection IDs

**Discovered**: 2026-01-30  
**Fixed**: 2026-01-31  
**Severity**: ğŸ”´ CRITICAL - Was causing complete data loss during sync  
**Status**: Root cause identified, fix pending implementation

#### Root Cause

When a `UnorderedMap`, `Vector`, or `UnorderedSet` is created via `::new()`, the underlying `Collection` generates a **random ID**:

```rust
// crates/storage/src/collections.rs
fn new_with_crdt_type(id: Option<Id>, crdt_type: CrdtType) -> Self {
    let id = id.unwrap_or_else(|| Id::random());  // â† THE BUG!
    // ...
}
```

This means:
- Node A creates `KvStore { items: UnorderedMap::new() }` â†’ `items` gets ID `0xABC123...`
- Node B creates `KvStore { items: UnorderedMap::new() }` â†’ `items` gets ID `0xDEF456...`

Even though both nodes have the same struct definition, they have **different collection IDs**.

#### Why This Breaks Sync

1. **Node A** stores entry at path: `compute_entry_id(0xABC123, "key1")` â†’ `0x111...`
2. **Node A** sends delta to **Node B**
3. **Node B** applies the delta - entry `0x111...` is stored correctly
4. **Node B** calls `items.get("key1")`
5. **Node B** looks up: `compute_entry_id(0xDEF456, "key1")` â†’ `0x222...` (DIFFERENT!)
6. **Result**: `None` - the data is there but at the wrong ID!

```
Node A storage:                      Node B storage after sync:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Root (0x00...)                â”‚    â”‚ Root (0x00...)                â”‚
â”‚ â””â”€ items (0xABC123)           â”‚    â”‚ â”œâ”€ items (0xDEF456) â† WRONG   â”‚
â”‚    â””â”€ key1 (0x111...)         â”‚    â”‚ â”‚  â””â”€ (nothing)               â”‚
â”‚       value: "hello"          â”‚    â”‚ â””â”€ entry (0x111...) â† ORPHAN  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    value: "hello"             â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The synced entry exists but is **orphaned** - the collection can't find it because it's looking under a different parent ID!

#### Manifestation in E2E Tests

```yaml
# Node 1 writes key_1_1 through key_1_10
# Node 2 writes key_2_1 through key_2_10
# After sync, both should have all 20 keys
# ACTUAL: get("key_2_1") returns null on Node 1 (and vice versa)
```

The logs showed:
- âœ… "Concurrent branch detected - applying with CRDT merge semantics"
- âœ… "Merge produced new hash (expected - concurrent branches merged)"
- âŒ But `get()` calls return `null`

### The Fix: Deterministic Collection IDs

**Inspiration**: `#[app::private]` already uses deterministic IDs based on field name!

```rust
// crates/storage/src/private.rs - EXISTING working pattern
fn compute_default_key<T>(name: &'static str) -> Key {
    let mut hasher = Sha256::new();
    hasher.update(name.as_bytes());
    Id::new(hasher.finalize().into())
}
```

#### Proposed Solution

1. **Modify `Collection::new_with_crdt_type`** to accept an optional field name:

```rust
fn new_with_crdt_type(id: Option<Id>, crdt_type: CrdtType, field_name: Option<&str>) -> Self {
    let id = id.unwrap_or_else(|| {
        if let Some(name) = field_name {
            // Deterministic ID from field name
            let mut hasher = Sha256::new();
            hasher.update(name.as_bytes());
            Id::new(hasher.finalize().into())
        } else {
            Id::random()  // Fallback for dynamic collections
        }
    });
    // ...
}
```

2. **Update `#[app::state]` macro** to pass field names:

```rust
// Instead of:
items: UnorderedMap::new()

// Generate:
items: UnorderedMap::new_with_field_name("items")
```

3. **Result**: All nodes agree on collection IDs without needing to sync!

```
Node A: KvStore.items â†’ SHA256("items") â†’ 0x5F3C4F85...
Node B: KvStore.items â†’ SHA256("items") â†’ 0x5F3C4F85... (SAME!)
```

### Test Evidence

```rust
#[test]
fn test_failure_mode_fresh_state_before_sync() {
    // Node A creates initial state
    let mut node_a_kv = Root::new(|| KvStore::init());
    node_a_kv.set("key1", "value_a").unwrap();
    let node_a_delta = get_last_delta();

    // Node B initializes fresh state (different collection ID!)
    let mut node_b_kv = Root::new(|| KvStore::init());
    
    // Apply Node A's delta
    Root::<KvStore>::sync(&delta).unwrap();

    // TRY TO READ - FAILS!
    assert_eq!(node_b_kv.get("key1"), Some("value_a")); // âŒ Returns None
}

#[test]
fn test_deterministic_collection_id_proposal() {
    // Compute deterministic ID from field name
    let id_a = SHA256("items");
    let id_b = SHA256("items");
    
    assert_eq!(id_a, id_b); // âœ… Always matches!
}
```

### Migration Considerations

1. **New nodes**: Use deterministic IDs (SHA256 of field name)
2. **Existing state**: Must be migrated or re-synced from scratch
3. **Genesis delta**: The very first delta establishes the collection IDs
4. **Backward compatibility**: Old states with random IDs won't work with new code

### Implementation Checklist

- [ ] Add `field_name: Option<&str>` parameter to `Collection::new_with_crdt_type`
- [ ] Create `UnorderedMap::new_with_field_name(&str)` constructor
- [ ] Update `#[app::state]` macro to generate deterministic collection creation
- [ ] Add migration tooling for existing contexts
- [ ] Update genesis delta handling to establish canonical IDs
- [ ] Add comprehensive tests for cross-node ID consistency

### Why This Wasn't Caught Earlier

1. **Single-node tests pass**: Collection IDs are consistent within a process
2. **Sync tests use serialized state**: When Node B deserializes Node A's state, it gets Node A's IDs
3. **The bug only manifests when**: Node B initializes fresh AND then receives deltas

The assumption was that nodes would either:
- Start from scratch and sync full state (works - gets correct IDs)
- Or be existing nodes with established IDs (works - IDs already correct)

But the failure case is:
- Node joins, initializes default state (wrong IDs), then receives deltas (data orphaned)

## Appendix I: Hash Mismatch Handling in CRDT Systems

### The Problem: Hash Rejection in Concurrent Systems

In the original implementation, when applying deltas, the system would:

1. Compare the delta's `expected_root_hash` with the computed hash after applying
2. **Reject the delta** if hashes didn't match in "sequential" scenarios

This caused problems in concurrent write scenarios:

```
Node A: genesis â†’ D1 (key_1) â†’ D2 (key_2) â†’ ...
Node B: genesis â†’ D1' (key_a) â†’ D2' (key_b) â†’ ...

When B receives D1, D2 from A:
- D1 is applied as merge (concurrent branch detected)
- D2's parent is D1, and we stored D1's computed hash
- D2 looks "sequential" because parent hash matches current hash
- BUT: D2 was designed for A's state (only A's keys)
- B's state has BOTH A's and B's keys
- Applying D2 produces hash X, but D2.expected_root_hash is Y
- Hash mismatch â†’ DELTA REJECTED â†’ sync fails
```

### The Root Cause

The delta's `expected_root_hash` is computed based on the **sender's linear history**. When the receiver has concurrent state (from their own writes or other nodes), applying the delta produces a **different hash** because the resulting state includes additional data.

This is **not a bug** - it's expected behavior in a CRDT system with concurrent writes!

### The Fix: Trust CRDT Semantics

In a CRDT environment, hash mismatches during delta application are **normal** and **expected**. The correct approach is:

```rust
// OLD (broken):
if !is_merge_scenario && computed_hash != expected_hash {
    return Err(RootHashMismatch);  // â† Rejects valid deltas!
}

// NEW (correct):
if computed_hash != expected_hash {
    // Log for debugging, but NEVER reject
    debug!("Hash mismatch (concurrent state) - CRDT merge ensures consistency");
}
// Always continue - CRDT semantics guarantee eventual consistency
```

### Why This Is Safe

1. **CRDT Guarantees**: CRDTs mathematically guarantee eventual consistency regardless of message order
2. **Merge Semantics**: All data operations use proper merge logic (LWW, counters, etc.)
3. **Hash Divergence is Temporary**: After all deltas are exchanged, hashes will converge
4. **No Data Loss**: Rejecting deltas causes data loss; accepting them preserves all data

### The Three Fixes Applied

1. **Deterministic Collection IDs** (Appendix H): Collections use `SHA256(field_name)` instead of random IDs
2. **Correct parent_hashes Storage**: Store `computed_hash` (actual result), not `expected_root_hash` (remote's expectation)
3. **Remove Hash Rejection**: Don't reject deltas based on hash mismatch; trust CRDT merge semantics

### Verification

The two-node concurrent write test (`crdt-merge.yml`) now passes:
- Both nodes write 10 unique keys each
- Both nodes successfully sync all 20 keys
- LWW conflict resolution works correctly (both nodes agree on winner)

## Appendix J: Dedicated Network Event Channel

### The Problem: Cross-Arbiter Message Loss

During three-node sync testing, we discovered a critical bug: **Node 2 received 40 StateDelta messages at the network layer but only processed 12**. The remaining 28 messages (including all 20 from Node 3) were silently lost.

#### Timeline of the Bug

```
23:53:00.369 - 23:53:00.499  Node 2 processes 12 StateDeltas from Node 1
23:53:00.499                 LAST message processed by NodeManager
23:53:00.505 - 23:53:00.546  Network dispatches 8 more StateDeltas (Node 1) - NOT HANDLED
23:53:00.550 - 23:53:00.927  Node 2 executes its own 20 writes (WASM/ContextManager)
23:53:00.937+               Network dispatches 20 StateDeltas from Node 3 - NEVER PROCESSED
```

#### Root Cause: Actix Cross-Arbiter Scheduling

The original architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BEFORE: LazyRecipient Approach                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   ARBITER A     â”‚         â”‚     ARBITER B       â”‚                    â”‚
â”‚  â”‚                 â”‚         â”‚                     â”‚                    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ do_send â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â”‚
â”‚  â”‚ â”‚ Network     â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚  NodeManager    â”‚ â”‚                    â”‚
â”‚  â”‚ â”‚ Manager     â”‚ â”‚ (cross- â”‚ â”‚                 â”‚ â”‚                    â”‚
â”‚  â”‚ â”‚             â”‚ â”‚ arbiter)â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚                    â”‚
â”‚  â”‚ â”‚ gossipsub â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”¼â”€â–ºâ”‚  mailbox  â”‚  â”‚ â”‚                    â”‚
â”‚  â”‚ â”‚ events     â”‚ â”‚         â”‚ â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚ â”‚                    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â”‚        â”‚       â”‚ â”‚                    â”‚
â”‚  â”‚                 â”‚         â”‚ â”‚        â–¼       â”‚ â”‚                    â”‚
â”‚  â”‚                 â”‚         â”‚ â”‚  handle(msg)   â”‚ â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚                 â”‚ â”‚                    â”‚
â”‚                              â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚                    â”‚
â”‚                              â”‚ â”‚ â”‚ ctx.spawn â”‚  â”‚ â”‚                    â”‚
â”‚                              â”‚ â”‚ â”‚ (futures) â”‚  â”‚ â”‚                    â”‚
â”‚                              â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚                    â”‚
â”‚                              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                          â”‚
â”‚  PROBLEM: When NodeManager is busy with spawned futures (WASM execution),â”‚
â”‚           incoming messages via do_send() are not processed promptly.    â”‚
â”‚           Under high load, this leads to effective message loss.         â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The `LazyRecipient<NetworkEvent>` sends messages across Actix arbiters using `do_send()`. When the receiving actor (NodeManager) is busy processing spawned async futures (e.g., WASM execution during local writes), incoming messages accumulate in the mailbox. Under high load with concurrent operations, this leads to messages being effectively lost.

### The Solution: Dedicated MPSC Channel

We replaced the `LazyRecipient` with a dedicated `tokio::sync::mpsc` channel and a bridge task:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AFTER: Dedicated Channel Approach                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚  â”‚   ARBITER A     â”‚                                                     â”‚
â”‚  â”‚                 â”‚                                                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   dispatch()    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ â”‚ Network     â”‚â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   MPSC Channel   â”‚               â”‚
â”‚  â”‚ â”‚ Manager     â”‚ â”‚   (non-block)   â”‚                  â”‚               â”‚
â”‚  â”‚ â”‚             â”‚ â”‚                 â”‚  - Size: 1000    â”‚               â”‚
â”‚  â”‚ â”‚ gossipsub   â”‚ â”‚                 â”‚  - Metrics       â”‚               â”‚
â”‚  â”‚ â”‚ events      â”‚ â”‚                 â”‚  - Backpressure  â”‚               â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”‚                 â”‚                          â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚ recv()                   â”‚
â”‚                                               â–¼                          â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                              â”‚      TOKIO TASK            â”‚              â”‚
â”‚                              â”‚   NetworkEventBridge       â”‚              â”‚
â”‚                              â”‚                            â”‚              â”‚
â”‚                              â”‚   loop {                   â”‚              â”‚
â”‚                              â”‚     event = rx.recv()      â”‚              â”‚
â”‚                              â”‚     node_manager.do_send() â”‚              â”‚
â”‚                              â”‚   }                        â”‚              â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                           â”‚ do_send()                    â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                              â”‚     ARBITER B              â”‚              â”‚
â”‚                              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚              â”‚
â”‚                              â”‚   â”‚  NodeManager    â”‚      â”‚              â”‚
â”‚                              â”‚   â”‚                 â”‚      â”‚              â”‚
â”‚                              â”‚   â”‚  handle(msg)    â”‚      â”‚              â”‚
â”‚                              â”‚   â”‚  ctx.spawn(...) â”‚      â”‚              â”‚
â”‚                              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚              â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                          â”‚
â”‚  SOLUTION: The bridge runs in its own tokio task, independent of actor   â”‚
â”‚            scheduling. Messages are guaranteed delivery or explicit drop.â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Components

#### 1. NetworkEventChannel (`crates/node/src/network_event_channel.rs`)

```rust
/// Configuration for the network event channel.
pub struct NetworkEventChannelConfig {
    /// Maximum number of events that can be buffered (default: 1000)
    pub channel_size: usize,
    
    /// Log warning when channel depth exceeds this percentage (default: 0.8)
    pub warning_threshold: f64,
    
    /// Interval for logging channel statistics (default: 30s)
    pub stats_log_interval: Duration,
}

/// Prometheus metrics for monitoring channel health
pub struct NetworkEventChannelMetrics {
    pub channel_depth: Gauge,           // Current events waiting
    pub events_received: Counter,       // Total sent to channel
    pub events_processed: Counter,      // Total received from channel
    pub events_dropped: Counter,        // Dropped due to full channel
    pub processing_latency: Histogram,  // Time from send to receive
}
```

Key features:
- **Configurable size**: Default 1000, handles burst patterns
- **Backpressure visibility**: Warning logs at 80% capacity
- **Metrics**: Prometheus metrics for monitoring
- **Graceful shutdown**: Drains remaining events before exit

#### 2. NetworkEventDispatcher Trait (`crates/network/primitives/src/messages.rs`)

```rust
/// Trait for dispatching network events.
/// Allows different mechanisms (channels, Actix recipients) to be used interchangeably.
pub trait NetworkEventDispatcher: Send + Sync {
    /// Dispatch a network event. Returns true if successful, false if dropped.
    fn dispatch(&self, event: NetworkEvent) -> bool;
}
```

#### 3. NetworkEventBridge (`crates/node/src/network_event_processor.rs`)

```rust
/// Bridge that forwards events from the channel to NodeManager.
pub struct NetworkEventBridge {
    receiver: NetworkEventReceiver,
    node_manager: Addr<NodeManager>,
    shutdown: Arc<Notify>,
}

impl NetworkEventBridge {
    pub async fn run(mut self) {
        loop {
            tokio::select! {
                event = self.receiver.recv() => {
                    match event {
                        Some(event) => self.node_manager.do_send(event),
                        None => break,  // Channel closed
                    }
                }
                _ = self.shutdown.notified() => break,
            }
        }
        self.graceful_shutdown();  // Drain remaining events
    }
}
```

### Why This Works

1. **Independent Scheduling**: The bridge runs in its own tokio task, not competing with actor message handling for scheduling time.

2. **Guaranteed Delivery or Explicit Drop**: Unlike `LazyRecipient::do_send()` which has no visibility into delivery, the channel:
   - Returns success/failure from `try_send()`
   - Logs warnings when dropping messages
   - Updates `events_dropped` metric

3. **Backpressure Visibility**: The metrics and logging show when the system is under pressure:
   ```
   WARN Network event channel approaching capacity current_depth=850 max_capacity=1000 fill_percent=85.0
   ```

4. **Same Processing Logic**: NodeManager's existing handlers and `ctx.spawn()` patterns are preserved. Only the delivery mechanism changes.

### Data Flow Comparison

| Step | Before (LazyRecipient) | After (Channel + Bridge) |
|------|------------------------|--------------------------|
| 1. Gossipsub event | NetworkManager receives | NetworkManager receives |
| 2. Dispatch | `lazy_recipient.do_send(event)` | `channel.dispatch(event)` â†’ `try_send()` |
| 3. Cross-thread | Actix scheduler (unreliable under load) | mpsc channel (guaranteed or drop) |
| 4. Receive | NodeManager's mailbox | Bridge's `rx.recv()` |
| 5. Forward to actor | (already in actor) | `node_manager.do_send(event)` |
| 6. Handle | `handle(msg, ctx)` | `handle(msg, ctx)` (same!) |

### Test Results

**Before (broken)**:
```
Three-node-convergence.yml: FAILED
- Node 1: 60 keys âœ“
- Node 2: 20 keys âœ— (missing 40 keys from peers)
- Node 3: 60 keys âœ“
```

**After (fixed)**:
```
Three-node-convergence.yml: PASSED
- Node 1: 60 keys âœ“
- Node 2: 60 keys âœ“
- Node 3: 60 keys âœ“
```

### Metrics for Production Monitoring

The channel exposes Prometheus metrics under `network_event_channel_*`:

| Metric | Type | Description |
|--------|------|-------------|
| `network_event_channel_depth` | Gauge | Current number of events in channel |
| `network_event_channel_received_total` | Counter | Total events sent to channel |
| `network_event_channel_processed_total` | Counter | Total events received from channel |
| `network_event_channel_dropped_total` | Counter | Events dropped due to full channel |
| `network_event_channel_processing_latency_seconds` | Histogram | Time from send to receive |

**Alert Recommendations**:
- Alert if `dropped_total` increases
- Alert if `depth` stays above 800 for >1 minute
- Monitor `processing_latency_seconds` p99

### Configuration

In production, the channel can be tuned via `NetworkEventChannelConfig`:

```rust
let channel_config = NetworkEventChannelConfig {
    channel_size: 1000,        // Increase for higher throughput
    warning_threshold: 0.8,    // Lower for earlier warnings
    stats_log_interval: Duration::from_secs(30),
};
```

For high-throughput deployments, consider:
- Increasing `channel_size` to 5000-10000
- Lowering `warning_threshold` to 0.7
- Adding alerts on `events_dropped`

### Lessons Learned

1. **Actix cross-arbiter messaging is not reliable under load**: When actors are busy with spawned futures, incoming messages can be effectively lost.

2. **Silent failures are dangerous**: The original `LazyRecipient::do_send()` provided no visibility into delivery failures.

3. **Dedicated channels for critical paths**: High-throughput message paths should use dedicated channels with explicit backpressure handling.

4. **Metrics are essential**: Without Prometheus metrics, this issue would have been nearly impossible to diagnose.

---

## Appendix K: Fresh Node Sync Strategy

### Problem

When a new node joins a context, it needs to bootstrap from peers. Two approaches exist:

| Approach | Mechanism | Speed | Use Case |
|----------|-----------|-------|----------|
| **Snapshot Sync** | Transfer full state in one request | Fast (~3ms) | Production, large state |
| **Delta Sync** | Fetch each delta from genesis | Slow (O(n) round trips) | Testing, debugging |

The optimal strategy depends on the deployment scenario, and switching between them required code changes.

### Solution: Configurable Fresh Node Strategy

Added `FreshNodeStrategy` enum and `--sync-strategy` CLI flag for easy benchmarking.

#### CLI Usage

```bash
# Fastest - single snapshot transfer (default)
merod --node-name node1 run --sync-strategy snapshot

# Slow - fetches all deltas from genesis (tests DAG path)
merod --node-name node1 run --sync-strategy delta

# Balanced - chooses based on peer state size
merod --node-name node1 run --sync-strategy adaptive

# Custom threshold - use snapshot if peer has â‰¥50 DAG heads
merod --node-name node1 run --sync-strategy adaptive:50
```

#### Strategy Enum

```rust
/// Strategy for syncing fresh (uninitialized) nodes.
pub enum FreshNodeStrategy {
    /// Always use snapshot sync (fastest, default)
    Snapshot,
    
    /// Always use delta-by-delta sync (slow, tests DAG)
    DeltaSync,
    
    /// Choose based on peer state size
    Adaptive {
        snapshot_threshold: usize,  // Default: 10
    },
}

impl FreshNodeStrategy {
    /// Determine if snapshot should be used based on peer's state.
    pub fn should_use_snapshot(&self, peer_dag_heads_count: usize) -> bool {
        match self {
            Self::Snapshot => true,
            Self::DeltaSync => false,
            Self::Adaptive { snapshot_threshold } => peer_dag_heads_count >= *snapshot_threshold,
        }
    }
}
```

#### Log Output

When a fresh node joins, you'll see:

```
INFO merod::cli::run: Using fresh node sync strategy fresh_node_strategy=snapshot
INFO calimero_node::sync::manager: Node needs sync, checking peer state 
    context_id=... is_uninitialized=true strategy=snapshot
INFO calimero_node::sync::manager: Peer has state, selecting sync strategy 
    peer_heads_count=1 use_snapshot=true strategy=snapshot
INFO calimero_node::sync::snapshot: Snapshot sync completed applied_records=6
```

### Benchmarking Guide

To compare strategies:

```bash
# Test 1: Snapshot sync (measure bootstrap time)
time merod --node-name fresh1 run --sync-strategy snapshot &
# Bootstrap time: ~3-5 seconds (mostly network setup)

# Test 2: Delta sync (measure with larger state)
time merod --node-name fresh2 run --sync-strategy delta &
# Bootstrap time: O(n) where n = number of deltas

# Test 3: Adaptive threshold tuning
merod --node-name fresh3 run --sync-strategy adaptive:5  # Small threshold
merod --node-name fresh4 run --sync-strategy adaptive:100  # Large threshold
```

---

## Appendix L: Snapshot Boundary Stubs

### Problem

After snapshot sync, a critical bug caused sync failures:

```
WARN calimero_node::handlers::state_delta: Delta pending due to missing parents
WARN calimero_node::sync::delta_request: Requested delta not found delta_id=[252, 46, ...]
```

**Root Cause**: Snapshot sync transfers the **state data** but NOT the **DAG history**.

```
Timeline:
1. Node 1 creates context â†’ genesis delta [fc2eb1e9...] with state
2. Node 3 joins â†’ snapshot sync transfers STATE + dag_heads=[fc2eb1e9...]
3. Node 3's DeltaStore is EMPTY (no actual delta objects!)
4. Node 1 writes â†’ creates new delta with parent=[fc2eb1e9...]
5. Node 3 receives delta â†’ can't find parent [fc2eb1e9...] in DeltaStore
6. Sync fails: "Delta pending due to missing parents"
```

### Solution: Snapshot Boundary Stubs

After snapshot sync, create "stub" deltas for each boundary `dag_head`:

```rust
/// Add boundary delta stubs to the DAG after snapshot sync.
///
/// Creates placeholder deltas for the snapshot boundary heads so that:
/// 1. New deltas referencing these heads as parents can be applied
/// 2. The DAG maintains correct topology
pub async fn add_snapshot_boundary_stubs(
    &self,
    boundary_dag_heads: Vec<[u8; 32]>,
    boundary_root_hash: [u8; 32],
) -> usize {
    let mut added_count = 0;
    let mut dag = self.dag.write().await;

    for head_id in boundary_dag_heads {
        // Skip genesis (zero hash)
        if head_id == [0; 32] {
            continue;
        }

        // Create a stub delta with no payload
        let stub = CausalDelta::new(
            head_id,
            vec![[0; 32]],    // Parent is "genesis" (we don't know actual parents)
            Vec::new(),       // Empty payload - no actions
            HybridTimestamp::default(),
            boundary_root_hash,  // Expected root hash is the snapshot boundary
        );

        // Restore the stub to the DAG (marks it as applied)
        if dag.restore_applied_delta(stub) {
            added_count += 1;
        }
    }
    added_count
}
```

### Integration in Snapshot Sync

The stubs are added after the snapshot is applied:

```rust
// In request_snapshot_sync_inner():

// 1. Transfer and apply snapshot pages
let applied_records = self.request_and_apply_snapshot_pages(...).await?;

// 2. Update context metadata
self.context_client.force_root_hash(&context_id, boundary.boundary_root_hash)?;
self.context_client.update_dag_heads(&context_id, boundary.dag_heads.clone())?;

// 3. CRITICAL: Add boundary stubs to DeltaStore
let delta_store = self.node_state.delta_stores.entry(context_id)...;
let stubs_added = delta_store
    .add_snapshot_boundary_stubs(
        boundary.dag_heads.clone(),
        *boundary.boundary_root_hash,
    )
    .await;

info!(%context_id, stubs_added, "Added snapshot boundary stubs to DeltaStore");
```

### Log Output (After Fix)

```
INFO calimero_node::sync::snapshot: Snapshot sync completed applied_records=6
INFO calimero_node::delta_store: Added snapshot boundary stub to DAG 
    context_id=... head_id=[133, 165, ...]
INFO calimero_node::delta_store: Snapshot boundary stubs added to DAG added_count=1
INFO calimero_node::sync::snapshot: Added snapshot boundary stubs to DeltaStore 
    stubs_added=1
```

### Test Results

| Test | Before Fix | After Fix |
|------|------------|-----------|
| `lww-conflict-resolution.yml` | âŒ Node 3 failed | âœ… All nodes synced |
| Fresh node receives delta | "Missing parents" error | Delta applied successfully |
| Snapshot bootstrap time | N/A (failed) | ~3ms |

### Why Stubs Work

1. **Stub ID matches boundary head**: When a new delta arrives with `parent=[fc2eb1e9...]`, the DAG finds the stub with matching ID.

2. **Stub marked as applied**: `dag.restore_applied_delta()` adds the stub to `self.applied`, so `can_apply()` returns true for children.

3. **Empty payload is safe**: The stub has no actions to apply - it's purely for parent resolution.

4. **Root hash preserved**: The stub's `expected_root_hash` matches the snapshot boundary, maintaining consistency.

### Edge Cases Handled

| Scenario | Behavior |
|----------|----------|
| Multiple dag_heads | One stub created per head |
| Genesis head `[0; 32]` | Skipped (always considered applied) |
| Stub already exists | `restore_applied_delta()` returns false, no duplicate |
| Empty dag_heads | No stubs created (loop exits immediately) |

---

## Appendix M: State Sync Strategy Configuration

### Overview

`StateSyncStrategy` controls which Merkle tree comparison protocol is used when nodes need to reconcile state. This is separate from `FreshNodeStrategy` which controls bootstrap behavior.

### Configuration

```rust
/// Strategy for Merkle tree state synchronization.
pub enum StateSyncStrategy {
    /// Auto-select based on tree characteristics (default)
    Adaptive,
    
    /// Standard recursive hash comparison
    HashComparison,
    
    /// Full state snapshot transfer
    Snapshot,
    
    /// Compressed snapshot (zstd)
    CompressedSnapshot,
    
    /// Bloom filter quick diff (for <10% divergence)
    BloomFilter { false_positive_rate: f32 },
    
    /// Subtree prefetch (for deep trees)
    SubtreePrefetch { max_depth: Option<usize> },
    
    /// Level-wise breadth-first (for wide shallow trees)
    LevelWise { max_depth: Option<usize> },
}
```

### CLI Usage

```bash
# Adaptive (default) - auto-select based on tree characteristics
merod run --state-sync-strategy adaptive

# Force specific protocols for testing/benchmarking
merod run --state-sync-strategy hash        # Standard recursive
merod run --state-sync-strategy snapshot    # Full transfer
merod run --state-sync-strategy compressed  # Compressed snapshot
merod run --state-sync-strategy bloom       # Bloom filter (1% FP)
merod run --state-sync-strategy bloom:0.05  # Bloom filter (5% FP)
merod run --state-sync-strategy subtree     # Subtree prefetch
merod run --state-sync-strategy subtree:5   # Max depth 5
merod run --state-sync-strategy level       # Level-wise
merod run --state-sync-strategy level:3     # Max depth 3
```

### Safety: Snapshot Protection for Initialized Nodes

**CRITICAL**: Snapshot sync would overwrite local data! Two layers of protection prevent this:

#### Layer 1: Adaptive Selection Never Returns Snapshot for Initialized Nodes

```
if !local_has_data:
    return CompressedSnapshot if remote_entities > 100 else Snapshot  // âœ… Safe

// ========================================================
// INITIALIZED NODE: NEVER use Snapshot - it would lose local changes!
// All protocols below use CRDT merge to preserve both sides.
// ========================================================

if divergence_ratio > 50% && remote_entities > 20:
    return HashComparison  // âœ… Uses CRDT merge (NOT Snapshot!)

if tree_depth > 3 && child_count < 10:
    return SubtreePrefetch

if remote_entities > 50 && divergence_ratio < 10%:
    return BloomFilter

if tree_depth <= 2 && child_count > 5:
    return LevelWise

return HashComparison  // Default
```

#### Layer 2: Runtime Safety Check in SyncManager

Even if explicitly configured via CLI (`--state-sync-strategy snapshot`):

```rust
if local_has_data {
    match selected {
        Snapshot | CompressedSnapshot => {
            warn!("SAFETY: Snapshot blocked for initialized node - using HashComparison");
            selected = HashComparison;
        }
    }
}
```

#### Safety Matrix

| Strategy | Fresh Node | Initialized Node |
|----------|-----------|------------------|
| **Snapshot** | âœ… Used | â›” **BLOCKED** â†’ HashComparison |
| **CompressedSnapshot** | âœ… Used | â›” **BLOCKED** â†’ HashComparison |
| **HashComparison** | âœ… Safe | âœ… **Uses CRDT merge** |
| **BloomFilter** | âœ… Safe | âœ… **Uses CRDT merge** |
| **SubtreePrefetch** | âœ… Safe | âœ… **Uses CRDT merge** |
| **LevelWise** | âœ… Safe | âœ… **Uses CRDT merge** |

### Protocol Comparison

| Protocol | Round Trips | Best For | Trade-offs |
|----------|-------------|----------|------------|
| **HashComparison** | O(depth) | General, any divergence | Multiple round trips |
| **Snapshot** | 1 | **Fresh nodes ONLY** | âš ï¸ Blocked for initialized nodes |
| **CompressedSnapshot** | 1 | **Fresh nodes ONLY** | âš ï¸ Blocked for initialized nodes |
| **BloomFilter** | 1-2 | Large tree, <10% diff | False positives |
| **SubtreePrefetch** | 2 | Deep trees, localized changes | Over-fetch risk |
| **LevelWise** | O(depth) | Wide shallow trees | High message count |

### Integration in SyncManager

The strategy is selected with safety checks:

```rust
// In SyncManager::select_state_sync_strategy
let mut selected = if configured.is_adaptive() {
    StateSyncStrategy::choose_protocol(...)
} else {
    configured
};

// SAFETY CHECK: Never use Snapshot on initialized nodes!
if local_has_data {
    match selected {
        Snapshot | CompressedSnapshot => {
            warn!("SAFETY: Snapshot blocked - using HashComparison");
            selected = HashComparison;
        }
    }
}
```

### Log Output

Normal selection:
```
INFO calimero_node::sync::manager: Selected state sync strategy
    context_id=...
    configured=adaptive
    selected=hash
    local_has_data=true
```

Safety block (when Snapshot is explicitly configured but blocked):
```
WARN calimero_node::sync::manager: SAFETY: Snapshot strategy blocked for initialized node 
    - using HashComparison to preserve local data
    context_id=...
    configured=snapshot
```

### Current Implementation Status

| Component | Status |
|-----------|--------|
| `StateSyncStrategy` enum | âœ… Implemented |
| CLI `--state-sync-strategy` flag | âœ… Implemented |
| Adaptive selection logic | âœ… Implemented |
| **Snapshot safety protection** | âœ… **Implemented (2 layers)** |
| Strategy logging | âœ… Implemented |
| Network-level BloomFilter | â³ Defined in storage tests only |
| Network-level SubtreePrefetch | â³ Defined in storage tests only |
| Network-level LevelWise | â³ Defined in storage tests only |

**Note**: All strategies (HashComparison, BloomFilter, SubtreePrefetch, LevelWise) are fully wired to the network layer:
- Network messages: `TreeNodeRequest`, `TreeNodeResponse`, `BloomFilterRequest`, `BloomFilterResponse`
- Dispatch: `SyncManager` calls `hash_comparison_sync()`, `bloom_filter_sync()` based on strategy
- Handlers: `handle_tree_node_request()`, `handle_bloom_filter_request()` respond to incoming requests

Current limitation: Underlying tree storage enumeration methods fall back to DAG sync for actual data transfer. The network protocol layer is complete.

### Running Isolated Strategy Tests

```bash
# Hash-Based Comparison
cargo test -p calimero-storage --lib network_sync_hash_based_minimal_diff -- --nocapture

# Snapshot Transfer
cargo test -p calimero-storage --lib network_sync_snapshot_fresh_node -- --nocapture

# Bloom Filter
cargo test -p calimero-storage --lib network_sync_bloom_filter_efficiency -- --nocapture

# Subtree Prefetch
cargo test -p calimero-storage --lib network_sync_subtree_prefetch_efficiency -- --nocapture

# Level-Wise
cargo test -p calimero-storage --lib network_sync_level_wise_efficiency -- --nocapture

# Comprehensive comparison
cargo test -p calimero-storage --lib network_sync_comprehensive_comparison -- --nocapture
```

---

## Appendix N: Sync Metrics and Observability

### Overview

Prometheus metrics and detailed timing logs have been added to provide observability into sync operations. This enables:

1. **Performance benchmarking** - Compare different sync strategies
2. **Debugging** - Identify slow syncs or failures
3. **Root cause analysis** - Per-phase timing breakdown
4. **Alerting** - Monitor sync health in production

### Prometheus Metrics

All metrics are registered under the `sync_` prefix:

#### Overall Sync Metrics

| Metric | Type | Description |
|--------|------|-------------|
| `sync_duration_seconds` | Histogram | Duration of sync operations (10ms to 5min buckets) |
| `sync_attempts_total` | Counter | Total sync attempts |
| `sync_successes_total` | Counter | Successful sync completions |
| `sync_failures_total` | Counter | Failed syncs (includes timeouts) |
| `sync_active` | Gauge | Currently active sync operations |
| `sync_snapshot_records_applied_total` | Counter | Records applied via snapshot sync |
| `sync_bytes_received_total` | Counter | Bytes received (uncompressed) |
| `sync_bytes_sent_total` | Counter | Bytes sent (uncompressed) |
| `sync_deltas_fetched_total` | Counter | Deltas fetched from peers |
| `sync_deltas_applied_total` | Counter | Deltas successfully applied |

#### Per-Phase Timing Metrics (NEW)

| Metric | Type | Description |
|--------|------|-------------|
| `sync_phase_peer_selection_seconds` | Histogram | Time selecting and connecting to peer |
| `sync_phase_key_share_seconds` | Histogram | Time for key share handshake |
| `sync_phase_dag_compare_seconds` | Histogram | Time comparing DAG state |
| `sync_phase_data_transfer_seconds` | Histogram | Time transferring data |
| `sync_phase_timeout_wait_seconds` | Histogram | Time waiting for timeouts |
| `sync_phase_merge_seconds` | Histogram | Time in merge operations |
| `sync_merge_operations_total` | Counter | Number of merge operations |
| `sync_hash_comparisons_total` | Counter | Number of hash comparisons |

### Log Markers

Two structured log markers are emitted for analysis:

#### `SYNC_PHASE_BREAKDOWN` - Per-phase timing for each sync

```
INFO calimero_node::sync::metrics: SYNC_PHASE_BREAKDOWN 
    context_id=...
    peer_id=12D3KooW...
    protocol=None
    peer_selection_ms="174.15"
    key_share_ms="2.09"
    dag_compare_ms="0.78"
    data_transfer_ms="0.00"
    timeout_wait_ms="0.00"
    merge_ms="0.00"
    merge_count=0
    hash_compare_count=0
    bytes_received=0
    bytes_sent=0
    total_ms="177.05"
```

#### `DELTA_APPLY_TIMING` - Per-delta apply timing with merge detection

```
INFO calimero_node::delta_store: DELTA_APPLY_TIMING
    context_id=...
    delta_id=[...]
    action_count=3
    final_root_hash=Hash("...")
    was_merge=true
    wasm_ms="2.40"
    total_ms="2.44"
```

### Extracting Metrics

Use the provided script to extract and analyze metrics from logs:

```bash
./scripts/extract-sync-metrics.sh <data_dir_prefix>

# Example:
./scripts/extract-sync-metrics.sh b3n10d

# Outputs:
# - Per-phase timing statistics (min, max, avg, P50, P95)
# - Tail latency analysis (flags P95/P50 > 2x)
# - Delta apply timing with merge statistics
# - Protocol distribution
# - CSV export: data/<prefix>_metrics/phase_stats.csv
# - Summary: data/<prefix>_metrics/summary.md
```

### PromQL Queries

```promql
# P95 peer selection time (root cause metric)
histogram_quantile(0.95, rate(sync_phase_peer_selection_seconds_bucket[5m]))

# Identify tail latency issues (P95/P50 > 2x)
histogram_quantile(0.95, rate(sync_phase_peer_selection_seconds_bucket[5m])) /
histogram_quantile(0.50, rate(sync_phase_peer_selection_seconds_bucket[5m])) > 2

# Sync success rate
sum(rate(sync_successes_total[5m])) / sum(rate(sync_attempts_total[5m]))

# Merge operations per minute
rate(sync_merge_operations_total[1m])

# P95 overall sync duration
histogram_quantile(0.95, rate(sync_duration_seconds_bucket[5m]))
```

### Implementation

Located in `crates/node/src/sync/metrics.rs`:

```rust
/// Per-phase timing breakdown for root cause analysis
#[derive(Debug, Clone, Default)]
pub struct SyncPhaseTimings {
    pub peer_selection_ms: f64,
    pub key_share_ms: f64,
    pub dag_compare_ms: f64,
    pub data_transfer_ms: f64,
    pub timeout_wait_ms: f64,
    pub merge_ms: f64,
    pub merge_count: u64,
    pub hash_compare_count: u64,
    pub bytes_received: u64,
    pub bytes_sent: u64,
    pub total_ms: f64,
}

/// Helper to time individual phases
pub struct PhaseTimer {
    start: Instant,
}

impl PhaseTimer {
    pub fn start() -> Self { Self { start: Instant::now() } }
    pub fn stop(&self) -> f64 { self.start.elapsed().as_secs_f64() * 1000.0 }
}
```

Usage in `SyncManager::initiate_sync_inner`:

```rust
let mut timings = SyncPhaseTimings::new();

// PHASE 1: Peer Selection
let phase_timer = PhaseTimer::start();
let mut stream = self.network_client.open_stream(chosen_peer).await?;
timings.peer_selection_ms = phase_timer.stop();

// PHASE 2: Key Share
let phase_timer = PhaseTimer::start();
self.initiate_key_share_process(...).await?;
timings.key_share_ms = phase_timer.stop();

// ... etc ...

// Log and record
timings.log(&context_id.to_string(), &peer_id.to_string(), &format!("{:?}", result));
self.metrics.record_phase_timings(&timings);
```

---

## Appendix O: Performance Analysis Findings

### Overview

This appendix documents proven performance characteristics based on instrumented benchmarks run on January 31, 2026.

### Key Finding: Peer Selection Dominates Sync Time

| Phase | P50 | P95 | % of Total |
|-------|-----|-----|------------|
| **peer_selection** | 174ms | 522ms | **99.4%** |
| key_share | 2.1ms | 4.8ms | 1.2% |
| dag_compare | 0.6ms | 1.4ms | 0.4% |
| data_transfer | 0ms | 0ms | 0% |
| **total_sync** | 175ms | 525ms | 100% |

**Root cause**: libp2p stream opening involves peer discovery/routing when not cached.

### Phase Timing Visualization

```
Sync Duration Breakdown (N=143 samples)
=======================================

                       P50 (ms)                    P95 (ms)
                       ========                    ========

peer_selection:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 174    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 522
key_share:             â–Œ 2.1                       â–Œ 4.8
dag_compare:           â– 0.6                       â– 1.4
data_transfer:         â– 0                         â– 0
                       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total_sync:            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 175    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 525


Phase Contribution (P50):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œâ–â–  â”‚
â”‚                         peer_selection (99.4%)              key (1%)  dag   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Delta Apply (WASM Merge) Performance

| Metric | P50 | P95 | Sample Size |
|--------|-----|-----|-------------|
| wasm_exec | 2.0ms | 2.4-6.6ms | N=70-100 |
| total_apply | 2.0ms | 2.6ms | N=100 |

**Finding**: Merges are O(n), not O(nÂ²). WASM execution time is stable regardless of conflict density.

### Merge Statistics by Scenario

| Scenario | Merge Ratio | Interpretation |
|----------|-------------|----------------|
| b3n10d (disjoint writes) | 25.7% | Concurrent writes cause merges |
| b3n50c (sequential conflicts) | ~0% | No true concurrency |
| b3nlj (late joiner) | 1.0% | Most deltas apply sequentially |

### Tail Latency Analysis

| Phase | P95/P50 Ratio | Status |
|-------|---------------|--------|
| peer_selection | 3.0x | âš ï¸ Structural variance |
| key_share | 2.3x | âš ï¸ Minor |
| dag_compare | 2.1x | âš ï¸ Minor |
| total_sync | 3.0x | âš ï¸ Driven by peer_selection |
| wasm_exec | 2.8x | âš ï¸ Occasional outliers |

**Interpretation**: P95/P50 > 2x across all phases indicates variance is inherent to libp2p networking, not a specific pathology.

### Optimization Recommendations

#### High Impact (based on findings)

1. **Peer connection caching/pooling** - First sync ~500ms, subsequent ~170ms
2. **Pre-establish streams to known peers** - Eliminate discovery latency
3. **Monitor `sync_phase_peer_selection_seconds{quantile="0.95"}`** - Primary health indicator

#### Low Priority (proven negligible)

1. Key share optimization - Only 2ms, already fast
2. DAG comparison optimization - Only 0.6ms, already fast  
3. Merge optimization - O(n), not a bottleneck

### Benchmark Commands

```bash
# Run benchmark workflow
python -m merobox.cli bootstrap run --no-docker \
  --binary-path ./target/release/merod \
  workflows/sync/bench-3n-10k-disjoint.yml

# Extract metrics
./scripts/extract-sync-metrics.sh b3n10d

# View summary
cat data/b3n10d_metrics/summary.md
```

### Related Documents

- `DEEP-SYNC-ANALYSIS.md` - Detailed analysis with all scenarios
- `MISSING_INSTRUMENTATION.md` - Instrumentation status and remaining gaps
- `BENCHMARK-RESULTS.md` - Raw benchmark data

---

## Copyright

Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
